# ===================== IMPORTS =====================
import os
import time
import pandas as pd
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import Pinecone
from pinecone import Pinecone as PineconeClient, PodSpec

# ===================== C·∫§U H√åNH =====================
load_dotenv(override=True)

OPENAI_API_KEY = os.getenv("OPENAI__API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI__EMBEDDING_MODEL")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME_MST", "excel-index") 

EXCEL_FOLDER = r"C:\Users\tabao\OneDrive\Desktop\cong_viec_lam\masothue"  
EMBEDDING_DIM = 3072
BATCH_SIZE = 50

# ===================== KH·ªûI T·∫†O =====================
print("üîß ƒêang kh·ªüi t·∫°o Pinecone v√† Embeddings...")

if not all([OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME]):
    print("‚ùå L·ªói: Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng (API key ho·∫∑c t√™n index)!")
    exit(1)

pc = PineconeClient(api_key=PINECONE_API_KEY)
emb = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=OPENAI_EMBEDDING_MODEL)

print("‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng!\n")

# ===================== H√ÄM H·ªñ TR·ª¢ =====================
def get_excel_files(folder_path: str) -> List[str]:
    """L·∫•y danh s√°ch file Excel/CSV"""
    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {folder_path}")
        return []

    files = []
    for f in os.listdir(folder_path):
        if f.lower().endswith((".xlsx", ".xls", ".csv")):
            files.append(os.path.join(folder_path, f))
    return files


def create_or_get_index(index_name: str) -> Any:
    """T·∫°o ho·∫∑c l·∫•y Pinecone index"""
    if index_name not in pc.list_indexes().names():
        print(f"üõ†Ô∏è T·∫°o m·ªõi Pinecone Index: {index_name}")
        pc.create_index(
            name=index_name,
            dimension=EMBEDDING_DIM,
            metric="cosine",
            spec=PodSpec(environment=PINECONE_ENVIRONMENT),
        )
        time.sleep(5)
    return pc.Index(index_name)


def load_and_chunk_excel(file_path: str) -> List[Document]:
    """ƒê·ªçc file Excel v√† chia nh·ªè n·ªôi dung"""
    filename = os.path.basename(file_path)
    print(f"üìÇ ƒêang ƒë·ªçc: {filename}")

    docs = []
    try:
        if file_path.lower().endswith(".csv"):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_excel(file_path)

        # Chuy·ªÉn m·ªói d√≤ng th√†nh Document
        for i, row in df.iterrows():
            text = "\n".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
            if text.strip():
                docs.append(Document(
                    page_content=text,
                    metadata={"source": filename, "row": i + 1}
                ))

        # Chunk n·ªôi dung
        splitter = RecursiveCharacterTextSplitter(
            chunk_size= 3000,
            chunk_overlap= 300,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        chunks = splitter.split_documents(docs)

        for i, d in enumerate(chunks):
            d.metadata["chunk_id"] = i

        print(f"‚úÖ {filename}: {len(chunks)} chunks t·ª´ {len(df)} h√†ng.")
        return chunks

    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc {filename}: {e}")
        return []


def upload_to_pinecone(all_docs: List[Document], index_name: str):
    """ƒê·∫©y d·ªØ li·ªáu l√™n Pinecone"""
    if not all_docs:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ n·∫°p.")
        return

    print(f"üöÄ ƒêang n·∫°p {len(all_docs)} documents v√†o Pinecone Index: {index_name}")
    index = create_or_get_index(index_name)

    total_batches = (len(all_docs) + BATCH_SIZE - 1) // BATCH_SIZE
    vectordb = None

    for i in range(0, len(all_docs), BATCH_SIZE):
        batch = all_docs[i : i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        print(f"   üì¶ Batch {batch_num}/{total_batches} ({len(batch)} docs)...", end=" ")

        if i == 0:
            vectordb = Pinecone.from_documents(batch, index_name=index_name, embedding=emb)
        else:
            vectordb.add_documents(batch)

        print("‚úì")
        time.sleep(1)

    print("‚úÖ Ho√†n t·∫•t ƒë·∫©y d·ªØ li·ªáu l√™n Pinecone!")


# ===================== MAIN =====================
if __name__ == "__main__":
    excel_files = get_excel_files(EXCEL_FOLDER)
    if not excel_files:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file Excel n√†o trong th∆∞ m·ª•c: {EXCEL_FOLDER}")
        exit(1)

    print(f"üìä T√¨m th·∫•y {len(excel_files)} file Excel:")
    for f in excel_files:
        print(f"   - {os.path.basename(f)}")
    print()

    all_docs = []
    for f in excel_files:
        chunks = load_and_chunk_excel(f)
        all_docs.extend(chunks)

    upload_to_pinecone(all_docs, PINECONE_INDEX_NAME)

    print("\nüéâ Ho√†n th√†nh n·∫°p d·ªØ li·ªáu Excel v√†o Pinecone!")
