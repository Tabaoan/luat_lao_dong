"""
Module x·ª≠ l√Ω truy v·∫•n tr·ª±c ti·∫øp file Excel v·ªÅ KCN/CCN
T√≠ch h·ª£p v√†o chatbot ƒë·ªÉ tr·∫£ v·ªÅ d·ªØ li·ªáu d·∫°ng JSON khi ng∆∞·ªùi d√πng h·ªèi
v·ªÅ s·ªë l∆∞·ª£ng ho·∫∑c danh s√°ch khu/c·ª•m c√¥ng nghi·ªáp.

‚úÖ B·ªî SUNG:
- Load industrial_zones.geojson (tu·ª≥ ch·ªçn) ƒë·ªÉ g·∫Øn t·ªça ƒë·ªô cho t·ª´ng KCN/CCN
- Tr·∫£ JSON c√≥ th√™m:
    - data[i]["coordinates"] = [lng, lat] (n·∫øu match ƒë∆∞·ª£c)
    - not_found_coordinates: danh s√°ch t√™n kh√¥ng match ƒë∆∞·ª£c t·ªça ƒë·ªô
"""

import pandas as pd
import re
import json
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

# RapidFuzz (khuy·∫øn ngh·ªã). N·∫øu kh√¥ng c√≥ s·∫Ω d√πng fallback match c∆° b·∫£n.
try:
    from rapidfuzz import fuzz, process
except Exception:
    fuzz = None
    process = None


class ExcelQueryHandler:
    def __init__(
        self,
        excel_path: str,
        geojson_path: Optional[str] = None,
        match_threshold: int = 82,
        llm=None
    ):
        """
        Kh·ªüi t·∫°o handler v·ªõi ƒë∆∞·ªùng d·∫´n file Excel

        Args:
            excel_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file Excel ch·ª©a th√¥ng tin KCN/CCN
            geojson_path: (tu·ª≥ ch·ªçn) ƒê∆∞·ªùng d·∫´n industrial_zones.geojson ƒë·ªÉ g·∫Øn t·ªça ƒë·ªô
            match_threshold: ng∆∞·ª°ng match t√™n (RapidFuzz) ƒë·ªÉ ch·∫•p nh·∫≠n t·ªça ƒë·ªô
            llm: Language model ƒë·ªÉ x·ª≠ l√Ω prompt-based (B·∫ÆT BU·ªòC)
        """
        self.excel_path = excel_path
        self.df: Optional[pd.DataFrame] = None
        self.llm = llm

        if not self.llm:
            print("‚ö†Ô∏è WARNING: H·ªá th·ªëng prompt-based c·∫ßn LLM. S·∫Ω fallback v·ªÅ keyword n·∫øu c·∫ßn.")

        self.match_threshold = match_threshold
        self.geojson_path = geojson_path

        # L∆∞u index map to·∫° ƒë·ªô: name_norm -> [lng, lat]
        self._iz_name_to_coord: Dict[str, List[float]] = {}
        self._iz_names_original: List[str] = []
        self._iz_names_norm: List[str] = []

        # Khai b√°o c√°c c·ªôt c·∫ßn thi·∫øt
        self.columns_map = {
            "province": None,
            "type": None,  # C·ªôt Lo·∫°i (KCN/CCN)
            "name": None,
            "address": None,
            "operation_time": None,
            "area": None,
            "rental_price": None,
            "industry": None
        }

        self._load_excel()
        self._load_geojson_if_provided()

    # ==========================================================
    # üß© LOAD FILE EXCEL & NH·∫¨N DI·ªÜN C·ªòT
    # ==========================================================
    def _load_excel(self):
        """Load file Excel v√† t·ª± ƒë·ªông ph√°t hi·ªán c√°c c·ªôt quan tr·ªçng"""
        try:
            self.df = pd.read_excel(self.excel_path)
            self.df.columns = self.df.columns.str.strip()

            for col in self.df.columns:
                col_lower = col.lower()
                if any(k in col_lower for k in ["t·ªânh", "th√†nh ph·ªë", "province"]):
                    self.columns_map["province"] = col
                elif any(k in col_lower for k in ["lo·∫°i", "loai", "type"]):
                    self.columns_map["type"] = col
                elif any(k in col_lower for k in ["t√™n", "ten", "kcn", "ccn"]) and "lo·∫°i" not in col_lower:
                    self.columns_map["name"] = col
                elif any(k in col_lower for k in ["ƒë·ªãa ch·ªâ", "dia chi", "address"]):
                    self.columns_map["address"] = col
                elif any(k in col_lower for k in ["th·ªùi gian", "v·∫≠n h√†nh", "operation"]):
                    self.columns_map["operation_time"] = col
                elif any(k in col_lower for k in ["di·ªán t√≠ch", "dien tich", "area"]):
                    self.columns_map["area"] = col
                elif any(k in col_lower for k in ["gi√° thu√™", "gia thue", "rent", "rental"]):
                    self.columns_map["rental_price"] = col
                elif any(k in col_lower for k in ["ng√†nh ngh·ªÅ", "nganh nghe", "industry"]):
                    self.columns_map["industry"] = col

            print(f"‚úÖ ƒê√£ load Excel: {len(self.df)} b·∫£n ghi")
            print("üß≠ C·∫•u tr√∫c c·ªôt nh·∫≠n di·ªán ƒë∆∞·ª£c:")
            for key, val in self.columns_map.items():
                print(f"   - {key}: {val}")

        except Exception as e:
            print(f"‚ùå L·ªói khi load Excel: {e}")
            self.df = None

    # ==========================================================
    # üó∫Ô∏è LOAD GEOJSON (industrial_zones.geojson) ƒë·ªÉ g·∫Øn t·ªça ƒë·ªô
    # ==========================================================
    def _load_geojson_if_provided(self):
        """
        Load GeoJSON n·∫øu c√≥ path.
        K·∫øt qu·∫£: map name_norm -> [lng, lat]
        """
        if not self.geojson_path:
            return

        p = Path(self.geojson_path)
        if not p.exists():
            print(f"‚ö†Ô∏è GeoJSON kh√¥ng t·ªìn t·∫°i: {self.geojson_path} (b·ªè qua g·∫Øn t·ªça ƒë·ªô)")
            return

        try:
            with open(p, "r", encoding="utf-8") as f:
                gj = json.load(f)

            features = gj.get("features", []) or []
            name_to_coord: Dict[str, List[float]] = {}

            iz_names_original: List[str] = []
            iz_names_norm: List[str] = []

            for fe in features:
                props = fe.get("properties", {}) or {}
                geom = fe.get("geometry", {}) or {}
                coords = geom.get("coordinates")

                name = str(props.get("name", "")).strip()
                if not name:
                    continue

                # Ch·ªâ h·ªó tr·ª£ Point [lng, lat] nh∆∞ file c·ªßa b·∫°n ƒëang d√πng
                if isinstance(coords, list) and len(coords) == 2 and all(isinstance(x, (int, float)) for x in coords):
                    n = self._normalize_text(name)
                    name_to_coord[n] = [float(coords[0]), float(coords[1])]
                    iz_names_original.append(name)
                    iz_names_norm.append(n)

            self._iz_name_to_coord = name_to_coord
            self._iz_names_original = iz_names_original
            self._iz_names_norm = iz_names_norm

            print(f"‚úÖ ƒê√£ load GeoJSON IZ: {len(self._iz_name_to_coord)} ƒëi·ªÉm c√≥ t·ªça ƒë·ªô")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói load GeoJSON: {e}. (b·ªè qua g·∫Øn t·ªça ƒë·ªô)")

    # ==========================================================
    # ü§ñ PROMPT-BASED QUERY ANALYSIS
    # ==========================================================
    def _analyze_query_with_llm(self, question: str) -> Dict[str, Any]:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch to√†n b·ªô c√¢u h·ªèi v√† tr·∫£ v·ªÅ th√¥ng tin c·∫ßn thi·∫øt
        
        Returns:
            {
                "is_industrial_query": bool,
                "province": str or None,
                "query_type": "KCN" | "CCN" | None (None = t·∫•t c·∫£),
                "search_type": "province" | "specific_name",
                "specific_name": str or None,
                "confidence": float,
                "reasoning": str
            }
        """
        if not self.llm or self.df is None:
            # Fallback v·ªÅ keyword n·∫øu kh√¥ng c√≥ LLM
            return self._fallback_keyword_analysis(question)
        
        # L·∫•y danh s√°ch t·ªânh c√≥ trong d·ªØ li·ªáu
        available_provinces = self.df[self.columns_map["province"]].dropna().unique().tolist()
        available_provinces_str = ", ".join(available_provinces)
        
        # L·∫•y m·ªôt s·ªë t√™n KCN/CCN m·∫´u ƒë·ªÉ LLM hi·ªÉu format
        sample_names = []
        if self.columns_map["name"] is not None:
            sample_names = self.df[self.columns_map["name"]].dropna().head(10).tolist()
        sample_names_str = ", ".join(sample_names[:5]) if sample_names else "Kh√¥ng c√≥ d·ªØ li·ªáu m·∫´u"
        
        prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch c√¢u h·ªèi v·ªÅ khu c√¥ng nghi·ªáp v√† c·ª•m c√¥ng nghi·ªáp Vi·ªát Nam.

DANH S√ÅCH T·ªàNH/TH√ÄNH PH·ªê C√ì D·ªÆ LI·ªÜU:
{available_provinces_str}

M·ªòT S·ªê T√äN KCN/CCN M·∫™U:
{sample_names_str}

C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: "{question}"

NHI·ªÜM V·ª§: Ph√¢n t√≠ch c√¢u h·ªèi v√† tr·∫£ v·ªÅ JSON v·ªõi c√°c th√¥ng tin sau:

1. "is_industrial_query": true/false
   - true n·∫øu c√¢u h·ªèi v·ªÅ khu c√¥ng nghi·ªáp (KCN) ho·∫∑c c·ª•m c√¥ng nghi·ªáp (CCN)
   - false n·∫øu kh√¥ng li√™n quan

2. "search_type": "province" ho·∫∑c "specific_name"
   - "province" n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ KCN/CCN trong m·ªôt t·ªânh/th√†nh ph·ªë
   - "specific_name" n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ m·ªôt KCN/CCN c·ª• th·ªÉ theo t√™n

3. "province": t√™n t·ªânh/th√†nh ph·ªë (ch·ªâ khi search_type = "province")
   - Tr√≠ch xu·∫•t t√™n t·ªânh t·ª´ c√¢u h·ªèi
   - Ph·∫£i kh·ªõp CH√çNH X√ÅC v·ªõi m·ªôt trong c√°c t·ªânh trong danh s√°ch
   - Tr·∫£ v·ªÅ null n·∫øu kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng kh·ªõp

4. "specific_name": t√™n KCN/CCN c·ª• th·ªÉ (ch·ªâ khi search_type = "specific_name")
   - Tr√≠ch xu·∫•t t√™n KCN/CCN t·ª´ c√¢u h·ªèi
   - Bao g·ªìm c·∫£ t·ª´ kh√≥a "KHU C√îNG NGHI·ªÜP" ho·∫∑c "C·ª§M C√îNG NGHI·ªÜP" n·∫øu c√≥

5. "query_type": lo·∫°i truy v·∫•n - QUAN TR·ªåNG: PH√ÇN BI·ªÜT R√ï R√ÄNG
   - "KCN" n·∫øu c√¢u h·ªèi CH·ªà NH·∫ÆC ƒê·∫æN "khu c√¥ng nghi·ªáp", "kcn", "khu cn", "khu" (v√† KH√îNG c√≥ "c·ª•m")
   - "CCN" n·∫øu c√¢u h·ªèi CH·ªà NH·∫ÆC ƒê·∫æN "c·ª•m c√¥ng nghi·ªáp", "ccn", "c·ª•m cn", "c·ª•m" (v√† KH√îNG c√≥ "khu")
   - null ch·ªâ khi c√¢u h·ªèi NH·∫ÆC ƒê·∫æN C·∫¢ HAI: "khu v√† c·ª•m", "kcn v√† ccn", "khu c√¥ng nghi·ªáp v√† c·ª•m c√¥ng nghi·ªáp"

6. "confidence": ƒë·ªô tin c·∫≠y (0.0-1.0)
   - M·ª©c ƒë·ªô ch·∫Øc ch·∫Øn v·ªÅ ph√¢n t√≠ch

7. "reasoning": l√Ω do ph√¢n t√≠ch
   - Gi·∫£i th√≠ch ng·∫Øn g·ªçn t·∫°i sao ph√¢n t√≠ch nh∆∞ v·∫≠y

QUAN TR·ªåNG - PH√ÇN BI·ªÜT QUERY_TYPE:
- N·∫øu c√¢u h·ªèi ch·ªâ c√≥ "khu" ho·∫∑c "kcn" (v√† KH√îNG c√≥ "c·ª•m") ‚Üí query_type = "KCN"
- N·∫øu c√¢u h·ªèi ch·ªâ c√≥ "c·ª•m" ho·∫∑c "ccn" (v√† KH√îNG c√≥ "khu") ‚Üí query_type = "CCN"  
- N·∫øu c√¢u h·ªèi c√≥ c·∫£ "khu" v√† "c·ª•m" ‚Üí query_type = null
- "c√¥ng nghi·ªáp" kh√¥ng quy·∫øt ƒë·ªãnh lo·∫°i, ch·ªâ c√≥ "khu" vs "c·ª•m" m·ªõi quy·∫øt ƒë·ªãnh
- LU√îN LU√îN ki·ªÉm tra xem c√¢u h·ªèi c√≥ c·∫£ "khu" v√† "c·ª•m" kh√¥ng tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh
- V√≠ d·ª•: "c·ª•m c√¥ng nghi·ªáp ·ªü Vƒ©nh Long" ‚Üí ch·ªâ c√≥ "c·ª•m", kh√¥ng c√≥ "khu" ‚Üí query_type = "CCN"
- V√≠ d·ª•: "khu c√¥ng nghi·ªáp ·ªü H√† N·ªôi" ‚Üí ch·ªâ c√≥ "khu", kh√¥ng c√≥ "c·ª•m" ‚Üí query_type = "KCN"

B∆Ø·ªöC PH√ÇN T√çCH QUERY_TYPE:
1. T√¨m t·ª´ "khu" ho·∫∑c "kcn" trong c√¢u h·ªèi ‚Üí has_khu = true/false
2. T√¨m t·ª´ "c·ª•m" ho·∫∑c "ccn" trong c√¢u h·ªèi ‚Üí has_cum = true/false  
3. N·∫øu has_khu = true v√† has_cum = true ‚Üí query_type = null
4. N·∫øu has_khu = true v√† has_cum = false ‚Üí query_type = "KCN"
5. N·∫øu has_khu = false v√† has_cum = true ‚Üí query_type = "CCN"
6. N·∫øu has_khu = false v√† has_cum = false ‚Üí query_type = null

V√ç D·ª§ SEARCH_TYPE = "province":
- "khu c√¥ng nghi·ªáp ·ªü H√† N·ªôi" ‚Üí {{"query_type": "KCN", "reasoning": "Ch·ªâ h·ªèi v·ªÅ KHU c√¥ng nghi·ªáp, kh√¥ng nh·∫Øc ƒë·∫øn c·ª•m"}}
- "c·ª•m c√¥ng nghi·ªáp ·ªü B√¨nh D∆∞∆°ng" ‚Üí {{"query_type": "CCN", "reasoning": "Ch·ªâ h·ªèi v·ªÅ C·ª§M c√¥ng nghi·ªáp, kh√¥ng nh·∫Øc ƒë·∫øn khu"}}
- "khu v√† c·ª•m c√¥ng nghi·ªáp ·ªü ƒê√† N·∫µng" ‚Üí {{"query_type": null, "reasoning": "H·ªèi v·ªÅ C·∫¢ HAI khu v√† c·ª•m"}}
- "danh s√°ch c·ª•m c√¥ng nghi·ªáp ·ªü B√¨nh D∆∞∆°ng" ‚Üí {{"query_type": "CCN", "reasoning": "Ch·ªâ h·ªèi v·ªÅ C·ª§M c√¥ng nghi·ªáp, kh√¥ng nh·∫Øc ƒë·∫øn khu"}}
- "v·∫Ω bi·ªÉu ƒë·ªì c·ª•m c√¥ng nghi·ªáp ·ªü H·∫£i Ph√≤ng" ‚Üí {{"query_type": "CCN", "reasoning": "Ch·ªâ h·ªèi v·ªÅ C·ª§M c√¥ng nghi·ªáp, kh√¥ng nh·∫Øc ƒë·∫øn khu"}}

V√ç D·ª§ SEARCH_TYPE = "specific_name":
- "cho t√¥i th√¥ng tin v·ªÅ KHU C√îNG NGHI·ªÜP NG≈® L·∫†C - Vƒ®NH LONG" ‚Üí {{"query_type": "KCN", "reasoning": "T√¨m KCN c·ª• th·ªÉ"}}
- "th√¥ng tin v·ªÅ c·ª•m c√¥ng nghi·ªáp ABC" ‚Üí {{"query_type": "CCN", "reasoning": "T√¨m CCN c·ª• th·ªÉ"}}

CH·ªà TR·∫¢ V·ªÄ JSON (kh√¥ng c√≥ markdown, kh√¥ng c√≥ text th√™m):
"""

        try:
            from langchain_core.messages import HumanMessage
            
            # Ki·ªÉm tra LLM c√≥ kh·∫£ d·ª•ng kh√¥ng
            if not hasattr(self.llm, 'invoke'):
                print("‚ö†Ô∏è LLM does not have invoke method")
                return self._fallback_keyword_analysis(question)
            
            # G·ªçi LLM v·ªõi error handling
            try:
                llm_response = self.llm.invoke([HumanMessage(content=prompt)])
                if not llm_response or not hasattr(llm_response, 'content'):
                    print("‚ö†Ô∏è LLM returned invalid response object")
                    return self._fallback_keyword_analysis(question)
                
                response = llm_response.content
                if not isinstance(response, str):
                    response = str(response)
                
                response = response.strip()
                
            except Exception as llm_error:
                print(f"‚ö†Ô∏è LLM invoke error: {llm_error}")
                return self._fallback_keyword_analysis(question)
            
            # Ki·ªÉm tra response c√≥ r·ªóng kh√¥ng
            if not response:
                print("‚ö†Ô∏è LLM returned empty response")
                return self._fallback_keyword_analysis(question)
            
            # Debug: In ra response ƒë·ªÉ ki·ªÉm tra (ch·ªâ khi c√≥ l·ªói)
            # print(f"üîç LLM raw response: '{response}'")
            
            # Th·ª≠ parse JSON
            import json
            try:
                result = json.loads(response)
            except json.JSONDecodeError as json_error:
                # Ch·ªâ log l·ªói n·∫øu response kh√¥ng r·ªóng
                if response.strip():
                    print(f"‚ö†Ô∏è JSON parse error: {json_error}")
                else:
                    print("‚ö†Ô∏è Empty response from LLM")
                    return self._fallback_keyword_analysis(question)
                
                # Th·ª≠ extract JSON t·ª´ response n·∫øu c√≥ markdown format
                import re
                
                # Lo·∫°i b·ªè markdown code blocks
                cleaned_response = response.strip()
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]  # B·ªè ```json
                if cleaned_response.startswith('```'):
                    cleaned_response = cleaned_response[3:]   # B·ªè ```
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]  # B·ªè ```
                
                cleaned_response = cleaned_response.strip()
                
                # Ki·ªÉm tra cleaned response c√≥ r·ªóng kh√¥ng
                if not cleaned_response:
                    print("‚ö†Ô∏è Cleaned response is empty")
                    return self._fallback_keyword_analysis(question)
                
                # Th·ª≠ parse l·∫°i
                try:
                    result = json.loads(cleaned_response)
                    # print("‚úÖ Successfully parsed cleaned JSON")
                except json.JSONDecodeError:
                    # Th·ª≠ t√¨m JSON object trong text
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
                    if json_match:
                        try:
                            result = json.loads(json_match.group())
                            # print("‚úÖ Successfully extracted JSON from response")
                        except:
                            print("‚ùå Failed to extract JSON from response")
                            return self._fallback_keyword_analysis(question)
                    else:
                        print("‚ùå No JSON found in response")
                        return self._fallback_keyword_analysis(question)
            
            # Validate result
            required_keys = ["is_industrial_query", "search_type", "province", "specific_name", "query_type", "confidence", "reasoning"]
            if not isinstance(result, dict):
                print(f"‚ö†Ô∏è LLM response is not a dict: {type(result)}")
                return self._fallback_keyword_analysis(question)
                
            if not all(key in result for key in required_keys):
                missing_keys = [key for key in required_keys if key not in result]
                print(f"‚ö†Ô∏è LLM response missing keys: {missing_keys}")
                return self._fallback_keyword_analysis(question)
            
            return result
                
        except Exception as e:
            print(f"‚ö†Ô∏è LLM analysis failed: {e}")
            return self._fallback_keyword_analysis(question)

    def _fallback_keyword_analysis(self, question: str) -> Dict[str, Any]:
        """Fallback keyword-based analysis khi LLM kh√¥ng kh·∫£ d·ª•ng"""
        question_norm = self._normalize_text(question.lower())
        
        # Check if industrial query
        industrial_keywords = [
            "kcn", "ccn", "khu cong nghiep", "cum cong nghiep",
            "khu cn", "cum cn", "khu nghiep", "cum nghiep"
        ]
        is_industrial = any(k in question_norm for k in industrial_keywords)
        
        if not is_industrial:
            return {
                "is_industrial_query": False,
                "search_type": "province",
                "province": None,
                "specific_name": None,
                "query_type": None,
                "confidence": 0.9,
                "reasoning": "Kh√¥ng ph·∫£i c√¢u h·ªèi v·ªÅ khu/c·ª•m c√¥ng nghi·ªáp"
            }
        
        # Extract province first (improved with TP.HCM recognition)
        province = None
        specific_name = None
        search_type = "province"
        
        if self.df is not None and self.columns_map["province"] is not None:
            unique_provinces = self.df[self.columns_map["province"]].dropna().unique()
            
            # Special handling for TP.HCM variations
            hcm_variations = [
                "thanh pho ho chi minh", "tp ho chi minh", "tp.hcm", "tphcm", 
                "ho chi minh", "hcm", "sai gon", "saigon"
            ]
            
            # Check for TP.HCM variations first
            for hcm_var in hcm_variations:
                if hcm_var in question_norm:
                    # Find the actual province name in data
                    for prov in unique_provinces:
                        prov_norm = self._normalize_text(str(prov).lower())
                        if "ho chi minh" in prov_norm or "hcm" in prov_norm:
                            province = str(prov)
                            break
                    if province:
                        break
            
            # If not TP.HCM, check other provinces - IMPROVED
            if not province:
                # First try exact match
                for prov in unique_provinces:
                    prov_norm = self._normalize_text(str(prov).lower())
                    if prov_norm in question_norm:
                        province = str(prov)
                        break
                
                # If no exact match, try partial match for common abbreviations
                if not province:
                    province_mappings = {
                        "ha noi": ["ha noi", "hanoi", "hn"],
                        "binh duong": ["binh duong", "bd"],
                        "dong nai": ["dong nai"], # Removed "dn" to avoid conflict
                        "bac ninh": ["bac ninh", "bn"],
                        "hai phong": ["hai phong", "haiphong", "hp"],
                        "da nang": ["da nang", "dn", "danang"], # Keep "dn" for Da Nang priority
                        "can tho": ["can tho", "cantho", "ct"],
                        "vinh phuc": ["vinh phuc", "vp"],
                        "thanh hoa": ["thanh hoa", "th"],
                        "nghe an": ["nghe an", "na"],
                        "quang ninh": ["quang ninh", "qn"],
                        "quang nam": ["quang nam"],
                        "long an": ["long an", "la"],
                        "bac giang": ["bac giang", "bg"],
                        "ba ria vung tau": ["ba ria vung tau", "brvt"],
                        "thua thien hue": ["thua thien hue", "tth"]
                    }
                    
                    # IMPROVED: Check for exact abbreviation matches first (higher priority)
                    for prov in unique_provinces:
                        prov_norm = self._normalize_text(str(prov).lower())
                        # Check if any abbreviation matches
                        for key, abbreviations in province_mappings.items():
                            if key in prov_norm:
                                for abbr in abbreviations:
                                    # Exact match for abbreviations to avoid conflicts
                                    if abbr == question_norm.strip() or f" {abbr} " in f" {question_norm} " or question_norm.endswith(f" {abbr}") or question_norm.startswith(f"{abbr} "):
                                        province = str(prov)
                                        break
                                    # Partial match for full names
                                    elif len(abbr) > 2 and abbr in question_norm:
                                        province = str(prov)
                                        break
                                if province:
                                    break
                        if province:
                            break
        
        # Determine search type based on patterns
        # Check for location indicators (province search) - IMPROVED
        location_indicators = ["o ", "tai ", "trong ", "tinh ", "thanh pho ", "danh sach", "list", "liet ke"]
        has_location_indicator = any(indicator in question_norm for indicator in location_indicators)
        
        # Check for specific name indicators
        specific_indicators = ["thong tin ve", "cho toi thong tin", "chi tiet ve", "detail", "ve khu cong nghiep", "ve cum cong nghiep"]
        has_specific_indicator = any(indicator in question_norm for indicator in specific_indicators)
        
        # IMPROVED Decision logic: 
        # 1. If we found a province name, it's likely a province search
        # 2. If we have location indicators, it's province search
        # 3. If it's a short query with industrial keywords + province, it's province search
        # 4. Only if we have specific indicators without province, it's specific name search
        
        if province:
            # We found a province, definitely province search
            search_type = "province"
            specific_name = None
        elif has_location_indicator:
            # Has location words like "·ªü", "t·∫°i" - province search
            search_type = "province"
            specific_name = None
        elif has_specific_indicator and not province:
            # Has specific indicators but no province found - specific name search
            search_type = "specific_name"
            # Try to extract the specific name (simplified)
            if "khu cong nghiep" in question_norm:
                # Find text after "khu cong nghiep"
                parts = question_norm.split("khu cong nghiep")
                if len(parts) > 1:
                    specific_name = f"khu cong nghiep{parts[1]}".strip()
            elif "cum cong nghiep" in question_norm:
                # Find text after "cum cong nghiep"
                parts = question_norm.split("cum cong nghiep")
                if len(parts) > 1:
                    specific_name = f"cum cong nghiep{parts[1]}".strip()
        else:
            # Default: if it's an industrial query, assume province search
            # This handles short queries like "KCN B√¨nh D∆∞∆°ng", "CCN HCM"
            search_type = "province"
            specific_name = None
        
        # Detect type (simplified) - C·∫¢I THI·ªÜN LOGIC
        has_cum = any(k in question_norm for k in ["cum", "ccn"])
        has_khu = any(k in question_norm for k in ["khu", "kcn"])
        
        # QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ lo·∫°i c·ª• th·ªÉ khi ch·ªâ c√≥ 1 lo·∫°i
        if has_cum and has_khu:
            query_type = None  # C√≥ c·∫£ hai
        elif has_cum and not has_khu:
            query_type = "CCN"  # Ch·ªâ c√≥ c·ª•m
        elif has_khu and not has_cum:
            query_type = "KCN"  # Ch·ªâ c√≥ khu
        else:
            query_type = None  # Kh√¥ng r√µ r√†ng
        
        return {
            "is_industrial_query": True,
            "search_type": search_type,
            "province": province,
            "specific_name": specific_name,
            "query_type": query_type,
            "confidence": 0.7,
            "reasoning": "Fallback keyword analysis"
        }

    def _generate_smart_error_message(self, question: str, extracted_province: Optional[str]) -> str:
        """T·∫°o th√¥ng b√°o l·ªói th√¥ng minh khi kh√¥ng t√¨m th·∫•y t·ªânh"""
        if not self.llm or self.df is None:
            return "‚ùì B·∫°n vui l√≤ng n√™u r√µ t·ªânh/th√†nh ph·ªë c·∫ßn tra c·ª©u."
        
        available_provinces = self.df[self.columns_map["province"]].dropna().unique().tolist()
        available_provinces_str = ", ".join(available_provinces)
        
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th√¥ng minh v·ªÅ d·ªØ li·ªáu khu c√¥ng nghi·ªáp Vi·ªát Nam.

DANH S√ÅCH T·ªàNH/TH√ÄNH PH·ªê C√ì D·ªÆ LI·ªÜU:
{available_provinces_str}

C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: "{question}"
T·ªàNH ƒê∆Ø·ª¢C TR√çCH XU·∫§T: "{extracted_province}"

NHI·ªÜM V·ª§: T·∫°o th√¥ng b√°o l·ªói th√¥ng minh v√† h·ªØu √≠ch:
1. Th√¥ng b√°o t·ªânh kh√¥ng c√≥ d·ªØ li·ªáu (n·∫øu c√≥ t·ªânh ƒë∆∞·ª£c tr√≠ch xu·∫•t)
2. G·ª£i √Ω 2-3 t·ªânh g·∫ßn nh·∫•t ho·∫∑c t∆∞∆°ng t·ª± c√≥ d·ªØ li·ªáu
3. Gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát

N·∫øu kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c t·ªânh n√†o, ch·ªâ c·∫ßn n√≥i "‚ùì B·∫°n vui l√≤ng n√™u r√µ t·ªânh/th√†nh ph·ªë c·∫ßn tra c·ª©u."

CH·ªà TR·∫¢ V·ªÄ TH√îNG B√ÅO B·∫∞NG TI·∫æNG VI·ªÜT:
"""

        try:
            from langchain_core.messages import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)]).content.strip()
            return response
        except Exception as e:
            print(f"‚ö†Ô∏è Error message generation failed: {e}")
            if extracted_province:
                return f"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho '{extracted_province}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n t·ªânh."
            else:
                return "‚ùì B·∫°n vui l√≤ng n√™u r√µ t·ªânh/th√†nh ph·ªë c·∫ßn tra c·ª©u."

    # ==========================================================
    # üß† NH·∫¨N DI·ªÜN C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG
    # ==========================================================
    def is_count_query(self, question: str) -> bool:
        """
        Nh·∫≠n di·ªán c√¢u h·ªèi v·ªÅ tra c·ª©u KCN/CCN (ƒë·∫øm, li·ªát k√™, danh s√°ch...).

        NOTE: b·∫£n c≈© ki·ªÉm tra count_keywords nh∆∞ng cu·ªëi c√πng v·∫´n return has_industrial.
        ·ªû ƒë√¢y gi·ªØ ‚Äútho√°ng‚Äù nh∆∞ng h·ª£p l√Ω h∆°n: c·∫ßn c√≥ industrial keyword.
        """
        analysis = self._analyze_query_with_llm(question)
        return analysis.get("is_industrial_query", False)

    # ==========================================================
    # üß≠ X√ÅC ƒê·ªäNH LO·∫†I TRUY V·∫§N (KHU / C·ª§M / C·∫¢ HAI)
    # ==========================================================
    def detect_type(self, question: str) -> Optional[str]:
        """
        X√°c ƒë·ªãnh ng∆∞·ªùi d√πng h·ªèi khu hay c·ª•m c√¥ng nghi·ªáp ho·∫∑c c·∫£ hai s·ª≠ d·ª•ng LLM analysis.
        """
        analysis = self._analyze_query_with_llm(question)
        return analysis.get("query_type")

    # ==========================================================
    # ü§ñ KI·ªÇM TRA T·ªàNH TH√îNG MINH V·ªöI LLM
    # ==========================================================
    def _smart_province_check(self, question: str, extracted_province: Optional[str]) -> Tuple[bool, str]:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ ki·ªÉm tra t·ªânh c√≥ t·ªìn t·∫°i trong d·ªØ li·ªáu hay kh√¥ng
        v√† ƒë∆∞a ra ph·∫£n h·ªìi th√¥ng minh
        
        Returns:
            (is_valid: bool, message: str)
        """
        if extracted_province is None:
            return False, "‚ùì B·∫°n vui l√≤ng n√™u r√µ t·ªânh/th√†nh ph·ªë c·∫ßn tra c·ª©u."
            
        if self.df is None:
            return False, "‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ tra c·ª©u."
        
        # L·∫•y danh s√°ch t·ªânh c√≥ trong d·ªØ li·ªáu
        available_provinces = self.df[self.columns_map["province"]].dropna().unique().tolist()
        
        # Ki·ªÉm tra exact match tr∆∞·ªõc
        province_normalized = self._normalize_text(extracted_province.lower())
        for available_province in available_provinces:
            if self._normalize_text(available_province.lower()) == province_normalized:
                return True, ""
        
        # Ki·ªÉm tra partial match
        for available_province in available_provinces:
            available_normalized = self._normalize_text(available_province.lower())
            if province_normalized in available_normalized or available_normalized in province_normalized:
                return True, ""
        
        # N·∫øu kh√¥ng c√≥ LLM, s·ª≠ d·ª•ng logic fallback ƒë∆°n gi·∫£n
        if not self.llm:
            # T√¨m t·ªânh g·∫ßn nh·∫•t
            similar_provinces = []
            for available_province in available_provinces:
                available_normalized = self._normalize_text(available_province.lower())
                # Ki·ªÉm tra c√≥ t·ª´ chung kh√¥ng
                province_words = set(province_normalized.split())
                available_words = set(available_normalized.split())
                if province_words.intersection(available_words):
                    similar_provinces.append(available_province)
            
            if similar_provinces:
                suggestion = f"C√≥ th·ªÉ b·∫°n mu·ªën t√¨m: {', '.join(similar_provinces[:3])}"
            else:
                # G·ª£i √Ω m·ªôt s·ªë t·ªânh ph·ªï bi·∫øn
                popular_provinces = [p for p in available_provinces if any(keyword in self._normalize_text(p.lower()) 
                                   for keyword in ['ha noi', 'ho chi minh', 'da nang', 'binh duong', 'dong nai'])][:3]
                if popular_provinces:
                    suggestion = f"M·ªôt s·ªë t·ªânh c√≥ d·ªØ li·ªáu: {', '.join(popular_provinces)}"
                else:
                    suggestion = f"M·ªôt s·ªë t·ªânh c√≥ d·ªØ li·ªáu: {', '.join(available_provinces[:3])}"
            
            return False, f"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho '{extracted_province}'. {suggestion}."
        
        # S·ª≠ d·ª•ng LLM n·∫øu c√≥
        available_provinces_str = ", ".join(available_provinces)
        
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω th√¥ng minh v·ªÅ d·ªØ li·ªáu khu c√¥ng nghi·ªáp Vi·ªát Nam.

DANH S√ÅCH T·ªàNH/TH√ÄNH PH·ªê C√ì D·ªÆ LI·ªÜU:
{available_provinces_str}

C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: "{question}"
T·ªàNH ƒê∆Ø·ª¢C TR√çCH XU·∫§T: "{extracted_province}"

NHI·ªÜM V·ª§:
1. Ki·ªÉm tra t·ªânh ƒë∆∞·ª£c tr√≠ch xu·∫•t c√≥ trong danh s√°ch kh√¥ng
2. N·∫øu KH√îNG c√≥, ƒë∆∞a ra ph·∫£n h·ªìi th√¥ng minh:
   - Th√¥ng b√°o t·ªânh kh√¥ng c√≥ d·ªØ li·ªáu
   - G·ª£i √Ω 2-3 t·ªânh g·∫ßn nh·∫•t ho·∫∑c t∆∞∆°ng t·ª± c√≥ d·ªØ li·ªáu
   - Gi·∫£i th√≠ch ng·∫Øn g·ªçn

ƒê·ªäNH D·∫†NG PH·∫¢N H·ªíI:
- N·∫øu t·ªânh C√ì trong danh s√°ch: tr·∫£ v·ªÅ "VALID"
- N·∫øu t·ªânh KH√îNG c√≥: tr·∫£ v·ªÅ th√¥ng b√°o chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát

CH·ªà TR·∫¢ V·ªÄ M·ªòT TRONG HAI:
- "VALID" (n·∫øu t·ªânh c√≥ d·ªØ li·ªáu)
- Th√¥ng b√°o chi ti·∫øt (n·∫øu t·ªânh kh√¥ng c√≥ d·ªØ li·ªáu)
"""

        try:
            from langchain_core.messages import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)]).content.strip()
            
            if response == "VALID":
                return True, ""
            else:
                return False, response
                
        except Exception as e:
            print(f"‚ö†Ô∏è LLM check failed: {e}")
            # Fallback v·ªÅ logic ƒë∆°n gi·∫£n ƒë√£ vi·∫øt ·ªü tr√™n
            similar_provinces = []
            for available_province in available_provinces:
                available_normalized = self._normalize_text(available_province.lower())
                province_words = set(province_normalized.split())
                available_words = set(available_normalized.split())
                if province_words.intersection(available_words):
                    similar_provinces.append(available_province)
            
            if similar_provinces:
                suggestion = f"C√≥ th·ªÉ b·∫°n mu·ªën t√¨m: {', '.join(similar_provinces[:3])}"
            else:
                popular_provinces = [p for p in available_provinces if any(keyword in self._normalize_text(p.lower()) 
                               for keyword in ['ha noi', 'ho chi minh', 'da nang', 'binh duong', 'dong nai'])][:3]
                if popular_provinces:
                    suggestion = f"M·ªôt s·ªë t·ªânh c√≥ d·ªØ li·ªáu: {', '.join(popular_provinces)}"
                else:
                    suggestion = f"M·ªôt s·ªë t·ªânh c√≥ d·ªØ li·ªáu: {', '.join(available_provinces[:3])}"
            
            return False, f"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho '{extracted_province}'. {suggestion}."

    # ==========================================================
    # üß© TR√çCH XU·∫§T T·ªàNH/TH√ÄNH PH·ªê - C·∫¢I THI·ªÜN
    # ==========================================================
    def extract_province(self, question: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t t√™n t·ªânh/th√†nh ph·ªë t·ª´ c√¢u h·ªèi s·ª≠ d·ª•ng LLM analysis."""
        analysis = self._analyze_query_with_llm(question)
        return analysis.get("province")

    # ==========================================================
    # üî° CHU·∫®N H√ìA TEXT (B·ªé D·∫§U)
    # ==========================================================
    def _normalize_text(self, text: str) -> str:
        intab = "√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë"
        outtab = "aaaaaaaaaaaaaaaaaeeeeeeeeeeeiiiiiooooooooooooooooouuuuuuuuuuuyyyyyd"
        intab_upper = "√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê"
        outtab_upper = "AAAAAAAAAAAAAAAAAEEEEEEEEEEEIIIIIOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYD"
        transtab = str.maketrans(intab + intab_upper, outtab + outtab_upper)
        return str(text).translate(transtab).lower().strip()

    # ==========================================================
    # üîç TRUY V·∫§N D·ªÆ LI·ªÜU
    # ==========================================================
    def query_by_province(self, province_name: str, query_type: Optional[str]) -> Optional[pd.DataFrame]:
        """
        L·ªçc d·ªØ li·ªáu theo t·ªânh/th√†nh ph·ªë v√† lo·∫°i (KCN/CCN).
        S·ª≠ d·ª•ng c·ªôt "Lo·∫°i" c√≥ s·∫µn trong Excel ƒë·ªÉ l·ªçc ch√≠nh x√°c.
        """
        if self.df is None or self.columns_map["province"] is None:
            return None

        # L·ªçc theo t·ªânh/th√†nh ph·ªë
        if province_name == "TO√ÄN QU·ªêC":
            df_filtered = self.df.copy()
        else:
            df_filtered = self.df[
                self.df[self.columns_map["province"]].astype(str).str.lower().str.contains(
                    str(province_name).lower(), na=False
                )
            ].copy()

        # L·ªçc theo lo·∫°i KCN/CCN d·ª±a v√†o c·ªôt "Lo·∫°i"
        if query_type and self.columns_map["type"] is not None:
            df_filtered = df_filtered[
                df_filtered[self.columns_map["type"]].astype(str).str.strip().str.upper() == query_type
            ]

        return df_filtered

    def query_by_specific_name(self, specific_name: str, query_type: Optional[str]) -> Optional[pd.DataFrame]:
        """
        T√¨m ki·∫øm KCN/CCN theo t√™n c·ª• th·ªÉ.
        S·ª≠ d·ª•ng fuzzy matching ƒë·ªÉ t√¨m t√™n g·∫ßn nh·∫•t.
        """
        if self.df is None or self.columns_map["name"] is None:
            return None

        specific_name_norm = self._normalize_text(specific_name.lower())
        
        # L·ªçc theo lo·∫°i KCN/CCN tr∆∞·ªõc n·∫øu c√≥
        df_to_search = self.df.copy()
        if query_type and self.columns_map["type"] is not None:
            df_to_search = df_to_search[
                df_to_search[self.columns_map["type"]].astype(str).str.strip().str.upper() == query_type
            ]

        # T√¨m ki·∫øm exact match tr∆∞·ªõc
        exact_matches = df_to_search[
            df_to_search[self.columns_map["name"]].astype(str).apply(
                lambda x: self._normalize_text(x.lower()) == specific_name_norm
            )
        ]
        
        if not exact_matches.empty:
            return exact_matches

        # T√¨m ki·∫øm partial match (contains)
        partial_matches = df_to_search[
            df_to_search[self.columns_map["name"]].astype(str).apply(
                lambda x: specific_name_norm in self._normalize_text(x.lower()) or 
                         self._normalize_text(x.lower()) in specific_name_norm
            )
        ]
        
        if not partial_matches.empty:
            return partial_matches

        # S·ª≠ d·ª•ng fuzzy matching n·∫øu c√≥ rapidfuzz
        if process is not None and fuzz is not None:
            all_names = df_to_search[self.columns_map["name"]].astype(str).tolist()
            if all_names:
                # T√¨m t√™n g·∫ßn nh·∫•t
                result = process.extractOne(specific_name, all_names, scorer=fuzz.WRatio)
                if result and result[1] >= 70:  # Threshold 70% cho t√™n KCN/CCN
                    best_match = result[0]
                    fuzzy_matches = df_to_search[
                        df_to_search[self.columns_map["name"]].astype(str) == best_match
                    ]
                    return fuzzy_matches

        # Kh√¥ng t√¨m th·∫•y
        return pd.DataFrame()

    # ==========================================================
    # üß≠ MATCH T·ªåA ƒê·ªò THEO T√äN KCN/CCN
    # ==========================================================
    def _match_coordinates(self, zone_name: str) -> Optional[List[float]]:
        """
        Tr·∫£ v·ªÅ [lng, lat] n·∫øu match ƒë∆∞·ª£c t√™n zone trong GeoJSON.
        """
        if not zone_name:
            return None
        if not self._iz_name_to_coord:
            return None

        z_norm = self._normalize_text(zone_name)

        # 1) exact match normalized
        if z_norm in self._iz_name_to_coord:
            return self._iz_name_to_coord[z_norm]

        # 2) fuzzy match n·∫øu c√≥ rapidfuzz
        if process is not None and fuzz is not None and self._iz_names_original:
            result = process.extractOne(zone_name, self._iz_names_original, scorer=fuzz.WRatio)
            if result and result[1] >= self.match_threshold:
                best_name = result[0]
                best_norm = self._normalize_text(best_name)
                return self._iz_name_to_coord.get(best_norm)

        # 3) fallback: contains match normalized (th√¥)
        for n, coord in self._iz_name_to_coord.items():
            if n and (n in z_norm or z_norm in n):
                return coord

        return None

    # ==========================================================
    # üßæ TR·∫¢ K·∫æT QU·∫¢ D·∫†NG JSON (dict ho·∫∑c string)
    # ==========================================================
    def format_json_response(
        self,
        df: pd.DataFrame,
        province_name: str,
        query_type: Optional[str],
        as_string: bool = True
    ) -> Any:
        """
        Tr·∫£ k·∫øt qu·∫£ truy v·∫•n d·∫°ng JSON.
        - as_string=True: tr·∫£ v·ªÅ chu·ªói JSON
        - as_string=False: tr·∫£ v·ªÅ dict (khuy·∫øn ngh·ªã khi d√πng trong Flask)
        """
        # C·∫£i thi·ªán label hi·ªÉn th·ªã
        if query_type == "KCN":
            label = "khu"
        elif query_type == "CCN":
            label = "c·ª•m"
        else:  # query_type is None - t·∫•t c·∫£
            label = "khu/c·ª•m"

        if df is None or df.empty:
            obj = {
                "province": province_name,
                "type": query_type,
                "count": 0,
                "message": f"Kh√¥ng t√¨m th·∫•y {label} c√¥ng nghi·ªáp t·∫°i {province_name}.",
                "data": [],
                "not_found_coordinates": []
            }
            return json.dumps(obj, ensure_ascii=False, indent=2) if as_string else obj

        cols = self.columns_map
        records = []
        not_found = []

        for _, row in df.iterrows():
            name_val = str(row.get(cols["name"], "")).strip()

            coord = self._match_coordinates(name_val)

            item = {
                "T·ªânh/Th√†nh ph·ªë": str(row.get(cols["province"], "")),
                "Lo·∫°i": str(row.get(cols["type"], "")),
                "T√™n": name_val,
                "ƒê·ªãa ch·ªâ": str(row.get(cols["address"], "")),
                "Th·ªùi gian v·∫≠n h√†nh": str(row.get(cols["operation_time"], "")),
                "T·ªïng di·ªán t√≠ch": str(row.get(cols["area"], "")),
                "Gi√° thu√™ ƒë·∫•t": str(row.get(cols["rental_price"], "")),
                "Ng√†nh ngh·ªÅ": str(row.get(cols["industry"], "")),
                # ‚úÖ B·ªî SUNG T·ªåA ƒê·ªò
                "coordinates": coord
            }

            if coord is None and name_val:
                not_found.append(name_val)

            records.append(item)

        # C·∫£i thi·ªán th√¥ng b√°o k·∫øt qu·∫£
        if query_type is None:  # T·∫•t c·∫£ lo·∫°i
            # ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ng lo·∫°i
            kcn_count = sum(1 for r in records if r.get("Lo·∫°i", "").upper() == "KCN")
            ccn_count = sum(1 for r in records if r.get("Lo·∫°i", "").upper() == "CCN")
            
            if kcn_count > 0 and ccn_count > 0:
                message = f"{province_name} c√≥ {kcn_count} khu c√¥ng nghi·ªáp v√† {ccn_count} c·ª•m c√¥ng nghi·ªáp."
            elif kcn_count > 0:
                message = f"{province_name} c√≥ {kcn_count} khu c√¥ng nghi·ªáp."
            elif ccn_count > 0:
                message = f"{province_name} c√≥ {ccn_count} c·ª•m c√¥ng nghi·ªáp."
            else:
                message = f"{province_name} c√≥ {len(records)} khu/c·ª•m c√¥ng nghi·ªáp."
        else:
            message = f"{province_name} c√≥ {len(records)} {label} c√¥ng nghi·ªáp."

        obj = {
            "province": province_name,
            "type": query_type,
            "count": len(records),
            "message": message,
            "data": records,
            "not_found_coordinates": not_found
        }

        return json.dumps(obj, ensure_ascii=False, indent=2) if as_string else obj

    def format_json_response_for_specific_name(
        self,
        df: pd.DataFrame,
        specific_name: str,
        query_type: Optional[str],
        as_string: bool = True
    ) -> Any:
        """
        Tr·∫£ k·∫øt qu·∫£ truy v·∫•n theo t√™n c·ª• th·ªÉ d·∫°ng JSON.
        - as_string=True: tr·∫£ v·ªÅ chu·ªói JSON
        - as_string=False: tr·∫£ v·ªÅ dict (khuy·∫øn ngh·ªã khi d√πng trong Flask)
        """
        # C·∫£i thi·ªán label hi·ªÉn th·ªã
        if query_type == "KCN":
            label = "khu"
        elif query_type == "CCN":
            label = "c·ª•m"
        else:  # query_type is None - t·∫•t c·∫£
            label = "khu/c·ª•m"

        if df is None or df.empty:
            obj = {
                "search_type": "specific_name",
                "specific_name": specific_name,
                "type": query_type,
                "count": 0,
                "message": f"Kh√¥ng t√¨m th·∫•y {label} c√¥ng nghi·ªáp v·ªõi t√™n '{specific_name}'.",
                "data": [],
                "not_found_coordinates": []
            }
            return json.dumps(obj, ensure_ascii=False, indent=2) if as_string else obj

        cols = self.columns_map
        records = []
        not_found = []

        for _, row in df.iterrows():
            name_val = str(row.get(cols["name"], "")).strip()

            coord = self._match_coordinates(name_val)

            item = {
                "T·ªânh/Th√†nh ph·ªë": str(row.get(cols["province"], "")),
                "Lo·∫°i": str(row.get(cols["type"], "")),
                "T√™n": name_val,
                "ƒê·ªãa ch·ªâ": str(row.get(cols["address"], "")),
                "Th·ªùi gian v·∫≠n h√†nh": str(row.get(cols["operation_time"], "")),
                "T·ªïng di·ªán t√≠ch": str(row.get(cols["area"], "")),
                "Gi√° thu√™ ƒë·∫•t": str(row.get(cols["rental_price"], "")),
                "Ng√†nh ngh·ªÅ": str(row.get(cols["industry"], "")),
                # ‚úÖ B·ªî SUNG T·ªåA ƒê·ªò
                "coordinates": coord
            }

            if coord is None and name_val:
                not_found.append(name_val)

            records.append(item)

        # T·∫°o th√¥ng b√°o k·∫øt qu·∫£ cho specific name search
        if len(records) == 1:
            message = f"T√¨m th·∫•y th√¥ng tin v·ªÅ '{specific_name}'."
        else:
            message = f"T√¨m th·∫•y {len(records)} k·∫øt qu·∫£ ph√π h·ª£p v·ªõi '{specific_name}'."

        obj = {
            "search_type": "specific_name",
            "specific_name": specific_name,
            "type": query_type,
            "count": len(records),
            "message": message,
            "data": records,
            "not_found_coordinates": not_found
        }

        return json.dumps(obj, ensure_ascii=False, indent=2) if as_string else obj

    # ==========================================================
    # ‚öôÔ∏è X·ª¨ L√ù TRUY V·∫§N NG∆Ø·ªúI D√ôNG
    # ==========================================================
    def process_query(self, question: str, return_json: bool = True, enable_rag: bool = False) -> Tuple[bool, Optional[Any]]:
        """
        X·ª≠ l√Ω truy v·∫•n v√† tr·∫£ k·∫øt qu·∫£ s·ª≠ d·ª•ng prompt-based analysis.
        H·ªó tr·ª£ c·∫£ t√¨m ki·∫øm theo t·ªânh v√† theo t√™n KCN/CCN c·ª• th·ªÉ.
        - return_json=True: tr·∫£ JSON (m·∫∑c ƒë·ªãnh)
            + tr·∫£ v·ªÅ STRING JSON (ƒë·ªÉ backward compatible)
        - return_json=False: tr·∫£ text b·∫£ng (nh∆∞ c≈©)
        - enable_rag=True: b·ªï sung RAG analysis

        Return:
            (handled: bool, response: Optional[str|dict])
        """
        # S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch to√†n b·ªô c√¢u h·ªèi m·ªôt l·∫ßn
        analysis = self._analyze_query_with_llm(question)
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i c√¢u h·ªèi v·ªÅ KCN/CCN kh√¥ng
        if not analysis.get("is_industrial_query", False):
            return False, None

        search_type = analysis.get("search_type", "province")
        province = analysis.get("province")
        specific_name = analysis.get("specific_name")
        query_type = analysis.get("query_type")
        
        # X·ª≠ l√Ω theo lo·∫°i t√¨m ki·∫øm
        if search_type == "specific_name":
            # T√¨m ki·∫øm theo t√™n KCN/CCN c·ª• th·ªÉ
            if specific_name is None:
                error_message = "‚ùì Vui l√≤ng cung c·∫•p t√™n KCN/CCN c·ª• th·ªÉ c·∫ßn t√¨m ki·∫øm."
                err = {"error": error_message}
                return True, json.dumps(err, ensure_ascii=False) if return_json else error_message
            
            # Truy v·∫•n d·ªØ li·ªáu theo t√™n c·ª• th·ªÉ
            df_result = self.query_by_specific_name(specific_name, query_type)
            
            if df_result is None or df_result.empty:
                error_message = f"‚ùå Kh√¥ng t√¨m th·∫•y KCN/CCN v·ªõi t√™n '{specific_name}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n ho·∫∑c th·ª≠ t√¨m theo t·ªânh/th√†nh ph·ªë."
                err = {"error": error_message}
                return True, json.dumps(err, ensure_ascii=False) if return_json else error_message
            
            # Tr·∫£ k·∫øt qu·∫£ cho specific name search
            if return_json:
                result = self.format_json_response_for_specific_name(df_result, specific_name, query_type, as_string=False)
                
                # ‚úÖ TH√äM RAG ANALYSIS CHO SPECIFIC NAME
                if enable_rag and isinstance(result, dict):
                    rag_analysis = self.enhance_list_with_rag(result, question)
                    if rag_analysis:
                        result["rag_analysis"] = rag_analysis
                        result["has_rag"] = True
                    else:
                        result["has_rag"] = False
                
                return True, json.dumps(result, ensure_ascii=False, indent=2)
            else:
                return True, self.format_table_response_for_specific_name(df_result, specific_name, query_type)
        
        else:
            # T√¨m ki·∫øm theo t·ªânh (logic c≈©)
            # Ki·ªÉm tra t·ªânh c√≥ h·ª£p l·ªá kh√¥ng
            if province is None:
                error_message = self._generate_smart_error_message(question, province)
                err = {"error": error_message}
                return True, json.dumps(err, ensure_ascii=False) if return_json else error_message
            
            # Ki·ªÉm tra t·ªânh c√≥ trong d·ªØ li·ªáu kh√¥ng
            is_valid, error_message = self._smart_province_check(question, province)
            if not is_valid:
                err = {"error": error_message}
                return True, json.dumps(err, ensure_ascii=False) if return_json else error_message

            # Truy v·∫•n d·ªØ li·ªáu theo t·ªânh
            df_result = self.query_by_province(province, query_type)

            if return_json:
                # ‚úÖ tr·∫£ dict ƒë·ªÉ c√≥ th·ªÉ th√™m RAG analysis
                result = self.format_json_response(df_result, province, query_type, as_string=False)
                
                # ‚úÖ TH√äM RAG ANALYSIS CHO PROVINCE QUERY
                if enable_rag and isinstance(result, dict):
                    rag_analysis = self.enhance_list_with_rag(result, question)
                    if rag_analysis:
                        result["rag_analysis"] = rag_analysis
                        result["has_rag"] = True
                    else:
                        result["has_rag"] = False
                
                return True, json.dumps(result, ensure_ascii=False, indent=2)
            else:
                return True, self.format_table_response(df_result, province, query_type)

    # ==========================================================
    # üß© GI·ªÆ L·∫†I H√ÄM C≈® (B·∫¢NG TEXT)
    # ==========================================================
    def format_table_response(self, df: pd.DataFrame, province_name: str, query_type: Optional[str]) -> str:
        """(Tu·ª≥ ch·ªçn) Hi·ªÉn th·ªã k·∫øt qu·∫£ d·∫°ng b·∫£ng text"""
        # C·∫£i thi·ªán label hi·ªÉn th·ªã
        if query_type == "KCN":
            label = "khu"
        elif query_type == "CCN":
            label = "c·ª•m"
        else:  # query_type is None - t·∫•t c·∫£
            label = "khu/c·ª•m"

        if df is None or df.empty:
            return f"Kh√¥ng t√¨m th·∫•y {label} c√¥ng nghi·ªáp t·∫°i {province_name}."

        cols = self.columns_map
        
        # C·∫£i thi·ªán th√¥ng b√°o k·∫øt qu·∫£ cho text response
        if query_type is None:  # T·∫•t c·∫£ lo·∫°i
            # ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ng lo·∫°i
            kcn_count = sum(1 for _, row in df.iterrows() if str(row.get(cols["type"], "")).upper() == "KCN")
            ccn_count = sum(1 for _, row in df.iterrows() if str(row.get(cols["type"], "")).upper() == "CCN")
            
            if kcn_count > 0 and ccn_count > 0:
                response = f"üìä {province_name} c√≥ {kcn_count} khu c√¥ng nghi·ªáp v√† {ccn_count} c·ª•m c√¥ng nghi·ªáp.\n\n"
            elif kcn_count > 0:
                response = f"üìä {province_name} c√≥ {kcn_count} khu c√¥ng nghi·ªáp.\n\n"
            elif ccn_count > 0:
                response = f"üìä {province_name} c√≥ {ccn_count} c·ª•m c√¥ng nghi·ªáp.\n\n"
            else:
                response = f"üìä {province_name} c√≥ {len(df)} khu/c·ª•m c√¥ng nghi·ªáp.\n\n"
        else:
            response = f"üìä {province_name} c√≥ {len(df)} {label} c√¥ng nghi·ªáp.\n\n"
            
        for _, row in df.iterrows():
            loai = str(row.get(cols['type'], '')).upper()
            ten = row.get(cols['name'], 'Kh√¥ng r√µ')
            dia_chi = row.get(cols['address'], '')
            response += f"- [{loai}] {ten} ({dia_chi})\n"
        return response

    def format_table_response_for_specific_name(self, df: pd.DataFrame, specific_name: str, query_type: Optional[str]) -> str:
        """(Tu·ª≥ ch·ªçn) Hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm theo t√™n c·ª• th·ªÉ d·∫°ng b·∫£ng text"""
        # C·∫£i thi·ªán label hi·ªÉn th·ªã
        if query_type == "KCN":
            label = "khu"
        elif query_type == "CCN":
            label = "c·ª•m"
        else:  # query_type is None - t·∫•t c·∫£
            label = "khu/c·ª•m"

        if df is None or df.empty:
            return f"Kh√¥ng t√¨m th·∫•y {label} c√¥ng nghi·ªáp v·ªõi t√™n '{specific_name}'."

        cols = self.columns_map
        
        # T·∫°o th√¥ng b√°o k·∫øt qu·∫£ cho specific name search
        if len(df) == 1:
            response = f"üìä T√¨m th·∫•y th√¥ng tin v·ªÅ '{specific_name}':\n\n"
        else:
            response = f"üìä T√¨m th·∫•y {len(df)} k·∫øt qu·∫£ ph√π h·ª£p v·ªõi '{specific_name}':\n\n"
            
        for _, row in df.iterrows():
            loai = str(row.get(cols['type'], '')).upper()
            ten = row.get(cols['name'], 'Kh√¥ng r√µ')
            dia_chi = row.get(cols['address'], '')
            tinh = row.get(cols['province'], '')
            response += f"- [{loai}] {ten} - {tinh} ({dia_chi})\n"
        return response

    # ==========================================================
    # üÜï IMPROVED KCN DETAIL QUERY WITH MULTIPLE CHOICE SUPPORT
    # ==========================================================
    
    def is_kcn_detail_query(self, question: str) -> bool:
        """
        Ki·ªÉm tra xem c√¢u h·ªèi c√≥ ph·∫£i l√† tra c·ª©u chi ti·∫øt KCN/CCN kh√¥ng
        
        QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ True cho c√°c c√¢u h·ªèi v·ªÅ KCN/CCN C·ª§ TH·ªÇ,
        KH√îNG ph·∫£i c√¢u h·ªèi v·ªÅ KCN/CCN theo t·ªânh/khu v·ª±c.
        
        V√ç D·ª§ DETAIL QUERY (True):
        - "th√¥ng tin v·ªÅ KCN VSIP B√¨nh D∆∞∆°ng"
        - "cho t√¥i bi·∫øt v·ªÅ CCN T√¢n B√¨nh"
        - "KCN Long H·∫≠u ·ªü ƒë√¢u"
        - "Detail KCN ABC"
        - "Khu c√¥ng nghi·ªáp VSIP" (ch·ªâ t√™n, kh√¥ng c√≥ location)
        
        V√ç D·ª§ KH√îNG PH·∫¢I DETAIL (False):
        - "KCN ·ªü B√¨nh D∆∞∆°ng" (province query)
        - "CCN t·∫°i Ngh·ªá An" (province query)
        - "danh s√°ch KCN ·ªü HCM" (list query)
        """
        question_lower = question.lower().strip()
        
        # 1. Ki·ªÉm tra t·ª´ kh√≥a "Detail" tr∆∞·ªõc - ∆∞u ti√™n cao nh·∫•t
        if question_lower.startswith('detail '):
            kcn_keywords = ['kcn', 'ccn', 'khu c√¥ng nghi·ªáp', 'c·ª•m c√¥ng nghi·ªáp']
            if any(keyword in question_lower for keyword in kcn_keywords):
                print(f"üéØ Detected Detail query (explicit): {question}")
                return True
        
        # 2. LO·∫†I TR·ª™ NGAY c√°c pattern province-based (QUAN TR·ªåNG)
        province_patterns = [
            r'(kcn|ccn|khu c√¥ng nghi·ªáp|c·ª•m c√¥ng nghi·ªáp)\s+(·ªü|t·∫°i|trong)\s+',  # "KCN ·ªü B√¨nh D∆∞∆°ng"
            r'(·ªü|t·∫°i|trong)\s+.*(kcn|ccn|khu c√¥ng nghi·ªáp|c·ª•m c√¥ng nghi·ªáp)',   # "·ªü HCM c√≥ KCN n√†o"
            r'danh s√°ch.*(kcn|ccn)',                                           # "danh s√°ch KCN"
            r'(c√≥ bao nhi√™u|s·ªë l∆∞·ª£ng).*(kcn|ccn)',                            # "c√≥ bao nhi√™u KCN"
            r'(kcn|ccn).*\s+(t·ªânh|th√†nh ph·ªë)',                                # "KCN t·ªânh B√¨nh D∆∞∆°ng"
            r'(c√°c|nh·ªØng)\s+(kcn|ccn)',                                       # "c√°c KCN"
            r'(kcn|ccn)\s+n√†o',                                               # "KCN n√†o"
            r'^(kcn|ccn)\s+(hcm|h√† n·ªôi|ƒë√† n·∫µng|b√¨nh d∆∞∆°ng|ngh·ªá an|b·∫Øc giang|b·∫Øc ninh|h·∫£i ph√≤ng|long an|ƒë·ªìng nai|vƒ©nh ph√∫c|thanh h√≥a)$',  # "CCN HCM"
        ]
        
        for pattern in province_patterns:
            if re.search(pattern, question_lower):
                print(f"‚ùå Rejected as province query: {question}")
                return False
        
        # 3. Ki·ªÉm tra c√≥ t·ª´ kh√≥a detail c·ª• th·ªÉ
        detail_keywords = [
            'th√¥ng tin v·ªÅ', 'cho t√¥i bi·∫øt v·ªÅ', 't√¨m hi·ªÉu v·ªÅ', 'gi·ªõi thi·ªáu v·ªÅ',
            'chi ti·∫øt v·ªÅ', 'm√¥ t·∫£ v·ªÅ', '·ªü ƒë√¢u', 'n·∫±m ·ªü ƒë√¢u', 'v·ªã tr√≠ c·ªßa',
            'ƒë·ªãa ch·ªâ c·ªßa', 'li√™n h·ªá', 'contact'
        ]
        
        has_detail_keyword = any(keyword in question_lower for keyword in detail_keywords)
        
        # 4. Pattern ƒë·∫∑c bi·ªát: "KCN/CCN + t√™n c·ª• th·ªÉ" (kh√¥ng c√≥ location indicators)
        # V√≠ d·ª•: "Khu c√¥ng nghi·ªáp VSIP", "CCN T√¢n Thu·∫≠n"
        simple_kcn_patterns = [
            r'^(khu c√¥ng nghi·ªáp|kcn)\s+(?!.*\b(·ªü|t·∫°i|trong)\b)[a-zA-Z√Ä-·ªπ0-9]+(?:\s+[a-zA-Z√Ä-·ªπ0-9\-]+)*\s*$',
            r'^(c·ª•m c√¥ng nghi·ªáp|ccn)\s+(?!.*\b(·ªü|t·∫°i|trong)\b)[a-zA-Z√Ä-·ªπ0-9]+(?:\s+[a-zA-Z√Ä-·ªπ0-9\-]+)*\s*$'
        ]
        
        # Ki·ªÉm tra pattern ƒë∆°n gi·∫£n tr∆∞·ªõc
        for pattern in simple_kcn_patterns:
            if re.match(pattern, question_lower):
                print(f"üéØ Detected simple KCN pattern: {question}")
                return True
        
        # 5. Ki·ªÉm tra c√≥ t√™n KCN/CCN c·ª• th·ªÉ v·ªõi detail keywords
        specific_name_patterns = [
            r'(khu c√¥ng nghi·ªáp|kcn)\s+([a-zA-Z√Ä-·ªπ0-9]+(?:\s+[a-zA-Z√Ä-·ªπ0-9\-]+)*)',  # Cho ph√©p t√™n c√≥ t·ªânh
            r'(c·ª•m c√¥ng nghi·ªáp|ccn)\s+([a-zA-Z√Ä-·ªπ0-9]+(?:\s+[a-zA-Z√Ä-·ªπ0-9\-]+)*)'   # Cho ph√©p t√™n c√≥ t·ªânh
        ]
        
        has_specific_name = False
        if has_detail_keyword:
            for pattern in specific_name_patterns:
                matches = re.findall(pattern, question_lower)
                if matches:
                    for match in matches:
                        # match[1] l√† t√™n sau KCN/CCN
                        name_part = match[1].strip()
                        # Ph·∫£i c√≥ √≠t nh·∫•t 1 t·ª´
                        if len(name_part.split()) >= 1:
                            has_specific_name = True
                            break
        
        # 6. Pattern ƒë·∫∑c bi·ªát: "KCN ABC ·ªü ƒë√¢u" - c√≥ t√™n c·ª• th·ªÉ + "·ªü ƒë√¢u"
        location_question_pattern = r'(khu c√¥ng nghi·ªáp|kcn|ccn)\s+([a-zA-Z√Ä-·ªπ0-9]+(?:\s+[a-zA-Z√Ä-·ªπ0-9\-]+)*)\s+·ªü\s+ƒë√¢u'
        location_match = re.search(location_question_pattern, question_lower)
        if location_match:
            name_part = location_match.group(2).strip()
            if len(name_part.split()) >= 1:  # √çt nh·∫•t 1 t·ª´ cho "·ªü ƒë√¢u" pattern
                has_specific_name = True
                has_detail_keyword = True
        
        # 7. Quy·∫øt ƒë·ªãnh cu·ªëi c√πng
        result = has_detail_keyword and has_specific_name
        
        if result:
            print(f"üéØ Detected KCN detail query: {question}")
        else:
            print(f"‚ùå Not a KCN detail query: {question}")
        
        return result

    def process_kcn_detail_query_with_multiple_choice(self, question: str) -> Optional[Dict]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi tra c·ª©u chi ti·∫øt KCN/CCN v·ªõi h·ªó tr·ª£ multiple choice
        
        Returns:
            - N·∫øu c√≥ 1 k·∫øt qu·∫£: {"type": "kcn_detail", "kcn_info": {...}, ...}
            - N·∫øu c√≥ nhi·ªÅu k·∫øt qu·∫£: {"type": "kcn_multiple_choice", "options": [...], ...}
            - N·∫øu kh√¥ng t√¨m th·∫•y: {"type": "kcn_detail_not_found", "message": "..."}
        """
        print(f"üîç Processing KCN detail query: {question}")
        
        if not self.is_kcn_detail_query(question):
            print("‚ùå Not a KCN detail query")
            return None
        
        # S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† tr√≠ch xu·∫•t t√™n KCN
        specific_name = None
        query_type = None
        
        if self.llm:
            print("ü§ñ Using LLM for analysis")
            analysis = self._analyze_query_with_llm(question)
            
            if not analysis.get("is_industrial_query", False):
                print("‚ùå LLM says not industrial query")
                return None
            
            if analysis.get("search_type") == "specific_name":
                specific_name = analysis.get("specific_name")
                query_type = analysis.get("query_type")
                print(f"üéØ LLM extracted: {specific_name}, type: {query_type}")
        
        # Fallback: extract name manually when no LLM or LLM failed
        if not specific_name:
            print("üîß Using fallback extraction")
            specific_name = self._extract_kcn_name_fallback(question)
            query_type = None  # Let query_by_specific_name handle this
            print(f"üéØ Fallback extracted: {specific_name}")
        
        if not specific_name:
            print("‚ùå Could not extract KCN name")
            return None
        
        # T√¨m th√¥ng tin KCN t·ª´ structured data
        print(f"üîç Searching for: {specific_name}")
        df_result = self.query_by_specific_name(specific_name, query_type)
        
        if df_result is None or df_result.empty:
            print(f"‚ùå No results found for: {specific_name}")
            return {
                "type": "kcn_detail_not_found",
                "message": f"Kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{specific_name}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n ho·∫∑c th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a kh√°c.",
                "query_name": specific_name
            }
        
        print(f"‚úÖ Found {len(df_result)} results")
        
        # üÜï KI·ªÇM TRA NHI·ªÄU K·∫æT QU·∫¢ TR√ôNG T√äN
        if len(df_result) > 1:
            print(f"üîÄ Multiple results found, creating choice list")
            return self._create_multiple_choice_response(df_result, specific_name, query_type)
        
        # Ch·ªâ c√≥ 1 k·∫øt qu·∫£ - tr·∫£ v·ªÅ chi ti·∫øt nh∆∞ c≈©
        return self._create_single_kcn_detail_response(df_result.iloc[0], specific_name, question)

    def _create_single_kcn_detail_response(self, row, specific_name: str, question: str) -> Dict:
        """
        T·∫°o response cho 1 KCN duy nh·∫•t
        """
        cols = self.columns_map
        
        kcn_info = {
            "T√™n": str(row.get(cols["name"], "")),
            "ƒê·ªãa ch·ªâ": str(row.get(cols["address"], "")),
            "T·ªânh/Th√†nh ph·ªë": str(row.get(cols["province"], "")),
            "Lo·∫°i": str(row.get(cols["type"], "")),
            "T·ªïng di·ªán t√≠ch": str(row.get(cols["area"], "")),
            "Gi√° thu√™ ƒë·∫•t": str(row.get(cols["rental_price"], "")),
            "Th·ªùi gian v·∫≠n h√†nh": str(row.get(cols["operation_time"], "")),
            "Ng√†nh ngh·ªÅ": str(row.get(cols["industry"], "")),
        }
        
        print(f"üìã KCN Info: {kcn_info['T√™n']}")
        
        # T√¨m t·ªça ƒë·ªô
        coordinates = self._match_coordinates(kcn_info["T√™n"])
        print(f"üìç Coordinates: {coordinates}")
        
        # Enhance v·ªõi RAG
        rag_analysis = self._enhance_with_rag(kcn_info, question)
        
        result = {
            "type": "kcn_detail",
            "kcn_info": kcn_info,
            "coordinates": coordinates,
            "zoom_level": 16,  # Zoom r·∫•t g·∫ßn ƒë·ªÉ th·∫•y chi ti·∫øt v·ªã tr√≠
            "matched_name": kcn_info["T√™n"],
            "query_name": specific_name,
            "message": f"Th√¥ng tin chi ti·∫øt v·ªÅ {kcn_info['T√™n']}"
        }
        
        # Th√™m RAG analysis n·∫øu c√≥
        if rag_analysis:
            result["rag_analysis"] = rag_analysis
            result["has_rag"] = True
            print("‚úÖ Added RAG analysis")
        else:
            result["has_rag"] = False
            print("‚ö†Ô∏è No RAG analysis")
        
        print("‚úÖ KCN detail query processed successfully")
        return result

    def _extract_kcn_name_fallback(self, question: str) -> Optional[str]:
        """
        Fallback method ƒë·ªÉ tr√≠ch xu·∫•t t√™n KCN/CCN khi kh√¥ng c√≥ LLM
        """
        import re
        
        question_clean = question.strip()
        
        # Pattern ƒë·∫∑c bi·ªát cho "Detail KCN/CCN [t√™n]"
        detail_match = re.search(r'detail\s+(kcn|ccn|khu c√¥ng nghi·ªáp|c·ª•m c√¥ng nghi·ªáp)\s+(.+?)(?:\s*$|\s*\?)', question_clean, re.IGNORECASE)
        if detail_match:
            kcn_type = detail_match.group(1).lower()
            kcn_name = detail_match.group(2).strip()
            if kcn_type in ['kcn', 'khu c√¥ng nghi·ªáp']:
                return f"khu c√¥ng nghi·ªáp {kcn_name}"
            else:
                return f"c·ª•m c√¥ng nghi·ªáp {kcn_name}"
        
        # Pattern 1: "v·ªÅ [t√™n KCN]"
        match = re.search(r'v·ªÅ\s+(.+?)(?:\s*$|\s*\?)', question_clean, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        
        # Pattern 2: Ch·ªâ c√≥ "KCN/CCN + t√™n" (pattern ƒë∆°n gi·∫£n)
        simple_patterns = [
            r'^(khu c√¥ng nghi·ªáp|kcn)\s+(.+?)(?:\s*$|\s*\?)',
            r'^(c·ª•m c√¥ng nghi·ªáp|ccn)\s+(.+?)(?:\s*$|\s*\?)'
        ]
        
        for pattern in simple_patterns:
            match = re.search(pattern, question_clean, re.IGNORECASE)
            if match:
                kcn_type = match.group(1).lower()
                kcn_name = match.group(2).strip()
                return f"{kcn_type} {kcn_name}"
        
        # Pattern 3: T√¨m t√™n c√≥ ch·ª©a KCN/CCN keywords trong c√¢u
        kcn_patterns = [
            r'(khu c√¥ng nghi·ªáp[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(kcn[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(c·ª•m c√¥ng nghi·ªáp[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(ccn[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)'
        ]
        
        for pattern in kcn_patterns:
            match = re.search(pattern, question_clean, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None

    def _create_multiple_choice_response(self, df_result: pd.DataFrame, specific_name: str, query_type: Optional[str]) -> Dict:
        """
        T·∫°o response khi c√≥ nhi·ªÅu KCN/CCN tr√πng t√™n ƒë·ªÉ ng∆∞·ªùi d√πng l·ª±a ch·ªçn
        """
        cols = self.columns_map
        options = []
        
        for idx, row in df_result.iterrows():
            kcn_name = str(row.get(cols["name"], ""))
            kcn_province = str(row.get(cols["province"], ""))
            kcn_address = str(row.get(cols["address"], ""))
            kcn_type = str(row.get(cols["type"], ""))
            
            # T√¨m t·ªça ƒë·ªô cho t·ª´ng option
            coordinates = self._match_coordinates(kcn_name)
            
            option = {
                "id": idx,  # ID ƒë·ªÉ ng∆∞·ªùi d√πng ch·ªçn
                "name": kcn_name,
                "province": kcn_province,
                "address": kcn_address,
                "type": kcn_type,
                "coordinates": coordinates,
                "display_text": f"{kcn_name} - {kcn_province}"
            }
            options.append(option)
        
        # T·∫°o message th√¥ng b√°o
        if query_type == "KCN":
            type_label = "khu c√¥ng nghi·ªáp"
        elif query_type == "CCN":
            type_label = "c·ª•m c√¥ng nghi·ªáp"
        else:
            type_label = "khu/c·ª•m c√¥ng nghi·ªáp"
        
        message = f"T√¨m th·∫•y {len(options)} {type_label} c√≥ t√™n t∆∞∆°ng t·ª± '{specific_name}'. Vui l√≤ng ch·ªçn m·ªôt trong c√°c t√πy ch·ªçn sau:"
        
        return {
            "type": "kcn_multiple_choice",  # Thay ƒë·ªïi type ƒë·ªÉ main.py x·ª≠ l√Ω
            "options": options,
            "message": message,
            "query_name": specific_name,
            "total_options": len(options)
        }

    def _enhance_with_rag(self, kcn_info: Dict, question: str) -> str:
        """
        S·ª≠ d·ª•ng RAG ƒë·ªÉ b·ªï sung th√¥ng tin chi ti·∫øt v·ªÅ KCN (simplified version)
        """
        if not self.llm:
            return ""
        
        try:
            # T·∫°o context t·ª´ structured data
            kcn_name = kcn_info.get('T√™n', 'N/A')
            kcn_address = kcn_info.get('ƒê·ªãa ch·ªâ', 'N/A')
            kcn_province = kcn_info.get('T·ªânh/Th√†nh ph·ªë', 'N/A')
            
            # T·∫°o enhanced query cho RAG
            rag_query = f"H√£y cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ {kcn_name} t·∫°i {kcn_province}. ƒê·ªãa ch·ªâ: {kcn_address}"
            
            # G·ªçi RAG system
            if hasattr(self.llm, 'invoke'):
                rag_response = self.llm.invoke(rag_query)
                if isinstance(rag_response, str):
                    return rag_response
                elif hasattr(rag_response, 'content'):
                    return rag_response.content
                else:
                    return str(rag_response)
            
            return ""
            
        except Exception as e:
            print(f"‚ö†Ô∏è RAG enhancement error: {e}")
            return ""

    def enhance_list_with_rag(self, query_result: Dict, question: str) -> str:
        """
        S·ª≠ d·ª•ng RAG ƒë·ªÉ b·ªï sung th√¥ng tin cho danh s√°ch KCN/CCN
        """
        if not self.llm:
            return ""
        
        try:
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ query result
            province = query_result.get('province', 'N/A')
            count = query_result.get('count', 0)
            query_type = query_result.get('type', 'N/A')
            
            # L·∫•y t√™n m·ªôt s·ªë KCN/CCN ti√™u bi·ªÉu
            data = query_result.get('data', [])
            sample_names = [item.get('T√™n', '') for item in data[:5]]
            sample_names_str = ', '.join(sample_names) if sample_names else 'N/A'
            
            # T·∫°o context-aware RAG query
            if query_type == "KCN":
                type_label = "khu c√¥ng nghi·ªáp"
            elif query_type == "CCN":
                type_label = "c·ª•m c√¥ng nghi·ªáp"
            else:
                type_label = "khu v√† c·ª•m c√¥ng nghi·ªáp"
            
            rag_query = f"""
Ph√¢n t√≠ch t√¨nh h√¨nh {type_label} t·∫°i t·ªânh {province}.

D·ªØ li·ªáu hi·ªÉn th·ªã {count} {type_label}, bao g·ªìm: {sample_names_str}

H√£y cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ:
1. T·ªïng quan v·ªÅ t√¨nh h√¨nh ph√°t tri·ªÉn {type_label} t·∫°i {province}
2. Ch√≠nh s√°ch ∆∞u ƒë√£i ƒë·∫ßu t∆∞ v√† thu h√∫t FDI c·ªßa t·ªânh
3. Ng√†nh ngh·ªÅ tr·ªçng ƒëi·ªÉm v√† l·ª£i th·∫ø c·∫°nh tranh
4. H·∫° t·∫ßng giao th√¥ng, logistics v√† k·∫øt n·ªëi v√πng
5. Ch·∫•t l∆∞·ª£ng ngu·ªìn nh√¢n l·ª±c v√† ƒë√†o t·∫°o
6. M√¥i tr∆∞·ªùng ƒë·∫ßu t∆∞ v√† th·ªß t·ª•c h√†nh ch√≠nh
7. K·∫ø ho·∫°ch ph√°t tri·ªÉn trong 5-10 nƒÉm t·ªõi
8. So s√°nh v·ªõi c√°c t·ªânh l√¢n c·∫≠n trong khu v·ª±c

C√¢u h·ªèi g·ªëc c·ªßa ng∆∞·ªùi d√πng: "{question}"

H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt v√† th·ª±c t·∫ø, t·∫≠p trung v√†o th√¥ng tin h·ªØu √≠ch cho nh√† ƒë·∫ßu t∆∞.
"""
            
            # G·ªçi RAG system
            if hasattr(self.llm, 'invoke'):
                rag_response = self.llm.invoke(rag_query)
                if isinstance(rag_response, str):
                    return rag_response
                elif hasattr(rag_response, 'content'):
                    return rag_response.content
                else:
                    return str(rag_response)
            
            return ""
            
        except Exception as e:
            print(f"‚ö†Ô∏è List RAG enhancement error: {e}")
            return ""

    def enhance_chart_with_rag(self, chart_data: Dict, question: str) -> str:
        """
        S·ª≠ d·ª•ng RAG ƒë·ªÉ b·ªï sung ph√¢n t√≠ch cho bi·ªÉu ƒë·ªì
        """
        if not self.llm:
            return ""
        
        try:
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ chart data
            province = chart_data.get('province', 'N/A')
            chart_type = chart_data.get('chart_type', 'N/A')
            data_count = len(chart_data.get('data', []))
            
            # T·∫°o context-aware RAG query
            rag_query = f"""
Ph√¢n t√≠ch bi·ªÉu ƒë·ªì {chart_type} v·ªÅ khu c√¥ng nghi·ªáp t·∫°i {province}.

D·ªØ li·ªáu hi·ªÉn th·ªã {data_count} khu c√¥ng nghi·ªáp.

H√£y cung c·∫•p ph√¢n t√≠ch chi ti·∫øt v·ªÅ:
1. T√¨nh h√¨nh ph√°t tri·ªÉn khu c√¥ng nghi·ªáp t·∫°i {province}
2. Ch√≠nh s√°ch ∆∞u ƒë√£i ƒë·∫ßu t∆∞ c·ªßa t·ªânh
3. Ng√†nh ngh·ªÅ tr·ªçng ƒëi·ªÉm v√† ti·ªÅm nƒÉng
4. H·∫° t·∫ßng giao th√¥ng v√† logistics
5. So s√°nh v·ªõi c√°c t·ªânh l√¢n c·∫≠n
6. Xu h∆∞·ªõng ph√°t tri·ªÉn trong t∆∞∆°ng lai
7. Ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ bi·ªÉu ƒë·ªì v√† ƒë∆∞a ra nh·∫≠n x√©t

C√¢u h·ªèi g·ªëc c·ªßa ng∆∞·ªùi d√πng: "{question}"

H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, t·∫≠p trung v√†o ph√¢n t√≠ch xu h∆∞·ªõng v√† c∆° h·ªôi ƒë·∫ßu t∆∞.
"""
            
            # G·ªçi RAG system
            if hasattr(self.llm, 'invoke'):
                rag_response = self.llm.invoke(rag_query)
                if isinstance(rag_response, str):
                    return rag_response
                elif hasattr(rag_response, 'content'):
                    return rag_response.content
                else:
                    return str(rag_response)
            
            return ""
            
        except Exception as e:
            print(f"‚ö†Ô∏è Chart RAG enhancement error: {e}")
            return ""


# ==========================================================
# üîå T√çCH H·ª¢P V√ÄO CHATBOT
# ==========================================================
def integrate_excel_to_chatbot(excel_path: str, geojson_path: Optional[str] = None, llm=None):
    """T√≠ch h·ª£p module Excel v√†o chatbot"""
    if not Path(excel_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file Excel: {excel_path}")
        return None
    handler = ExcelQueryHandler(excel_path, geojson_path=geojson_path, llm=llm)
    print("‚úÖ ƒê√£ t√≠ch h·ª£p module truy v·∫•n Excel v·ªõi LLM support.")
    return handler


# ==========================================================
# üß™ TEST MODULE
# ==========================================================
if __name__ == "__main__":
    EXCEL_FILE = r"./data/IIPMap_FULL_63_COMPLETE.xlsx"
    GEOJSON_FILE = r"./map_ui/industrial_zones.geojson"  

    # Kh·ªüi t·∫°o LLM cho test
    try:
        from langchain_openai import ChatOpenAI
        test_llm = ChatOpenAI(
            model_name="gpt-4o-mini",
            temperature=0
        )
        print("‚úÖ LLM initialized for testing")
    except:
        test_llm = None
        print("‚ö†Ô∏è LLM not available for testing")

    handler = ExcelQueryHandler(EXCEL_FILE, geojson_path=GEOJSON_FILE, llm=test_llm)

    test_queries = [
        "Danh s√°ch c·ª•m c√¥ng nghi·ªáp ·ªü B·∫Øc Ninh",
        "Danh s√°ch khu c√¥ng nghi·ªáp ·ªü B·∫Øc Ninh",
        "Danh s√°ch khu v√† c·ª•m c√¥ng nghi·ªáp ·ªü B·∫Øc Ninh",
        "Danh s√°ch t·∫•t c·∫£ khu c√¥ng nghi·ªáp v√† c·ª•m c√¥ng nghi·ªáp ·ªü H√† N·ªôi",
        "V·∫Ω bi·ªÉu ƒë·ªì c·ªôt v·ªÅ di·ªán t√≠ch c·ªßa khu c√¥ng nghi·ªáp ·ªü H·ªì Ch√≠ Minh",
        "V·∫Ω bi·ªÉu ƒë·ªì c·ªôt v·ªÅ di·ªán t√≠ch c·ªßa c·ª•m c√¥ng nghi·ªáp ·ªü ƒê√† N·∫µng",
        "V·∫Ω bi·ªÉu ƒë·ªì c·ªôt v·ªÅ di·ªán t√≠ch c·ªßa c·∫£ khu v√† c·ª•m c√¥ng nghi·ªáp ·ªü B√¨nh D∆∞∆°ng",
        "Khu v√† c·ª•m c√¥ng nghi·ªáp t·ªânh Lai Ch√¢u",  # Test t·ªânh kh√¥ng c√≥ d·ªØ li·ªáu
        "Danh s√°ch khu c√¥ng nghi·ªáp ·ªü ƒêi·ªán Bi√™n",  # Test t·ªânh kh√¥ng c√≥ d·ªØ li·ªáu
        # Test specific name searches
        "cho t√¥i th√¥ng tin v·ªÅ KHU C√îNG NGHI·ªÜP NG≈® L·∫†C - Vƒ®NH LONG",
        "th√¥ng tin v·ªÅ khu c√¥ng nghi·ªáp S√≥ng Th·∫ßn",
        "t√¨m c·ª•m c√¥ng nghi·ªáp T√¢n B√¨nh",
        "KHU C√îNG NGHI·ªÜP VSIP B·∫ÆC NINH",
        "c·ª•m c√¥ng nghi·ªáp Ph√∫ M·ªπ"
    ]

    print("\n" + "=" * 80)
    print("TEST MODULE TR·∫¢ K·∫æT QU·∫¢ D·∫†NG JSON (C√ì T·ªåA ƒê·ªò + LLM SMART CHECK)")
    print("=" * 80)

    for query in test_queries:
        print(f"\n‚ùì {query}")
        handled, response = handler.process_query(query, return_json=True)
        if handled:
            print(response)
        else:
            print("‚è≠Ô∏è B·ªè qua - Kh√¥ng ph·∫£i c√¢u h·ªèi li·ªát k√™ KCN/CCN ho·∫∑c thi·∫øu th√¥ng tin")
        print("-" * 80)

    # ==========================================================
    # üÜï MULTIPLE CHOICE SUPPORT FOR KCN DETAIL QUERIES
    # ==========================================================
    
    def _create_multiple_choice_response(self, df_result: pd.DataFrame, specific_name: str, query_type: Optional[str]) -> Dict:
        """
        T·∫°o response khi c√≥ nhi·ªÅu KCN/CCN tr√πng t√™n ƒë·ªÉ ng∆∞·ªùi d√πng l·ª±a ch·ªçn
        """
        cols = self.columns_map
        options = []
        
        for idx, row in df_result.iterrows():
            kcn_name = str(row.get(cols["name"], ""))
            kcn_province = str(row.get(cols["province"], ""))
            kcn_address = str(row.get(cols["address"], ""))
            kcn_type = str(row.get(cols["type"], ""))
            
            # T√¨m t·ªça ƒë·ªô cho t·ª´ng option
            coordinates = self._match_coordinates(kcn_name)
            
            option = {
                "id": idx,  # ID ƒë·ªÉ ng∆∞·ªùi d√πng ch·ªçn
                "name": kcn_name,
                "province": kcn_province,
                "address": kcn_address,
                "type": kcn_type,
                "coordinates": coordinates,
                "display_text": f"{kcn_name} - {kcn_province}"
            }
            options.append(option)
        
        # T·∫°o message th√¥ng b√°o
        if query_type == "KCN":
            type_label = "khu c√¥ng nghi·ªáp"
        elif query_type == "CCN":
            type_label = "c·ª•m c√¥ng nghi·ªáp"
        else:
            type_label = "khu/c·ª•m c√¥ng nghi·ªáp"
        
        message = f"T√¨m th·∫•y {len(options)} {type_label} c√≥ t√™n t∆∞∆°ng t·ª± '{specific_name}'. Vui l√≤ng ch·ªçn m·ªôt trong c√°c t√πy ch·ªçn sau:"
        
        return {
            "type": "kcn_multiple_choice",  # Thay ƒë·ªïi type ƒë·ªÉ main.py x·ª≠ l√Ω
            "options": options,
            "message": message,
            "query_name": specific_name,
            "total_options": len(options)
        }

    def process_kcn_detail_query(self, question: str) -> Optional[Dict]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi tra c·ª©u chi ti·∫øt KCN/CCN v·ªõi h·ªó tr·ª£ multiple choice
        """
        print(f"üîç Processing KCN detail query: {question}")
        
        if not self.is_kcn_detail_query(question):
            print("‚ùå Not a KCN detail query")
            return None
        
        # S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† tr√≠ch xu·∫•t t√™n KCN
        specific_name = None
        query_type = None
        
        if self.llm:
            print("ü§ñ Using LLM for analysis")
            analysis = self._analyze_query_with_llm(question)
            
            if not analysis.get("is_industrial_query", False):
                print("‚ùå LLM says not industrial query")
                return None
            
            if analysis.get("search_type") == "specific_name":
                specific_name = analysis.get("specific_name")
                query_type = analysis.get("query_type")
                print(f"üéØ LLM extracted: {specific_name}, type: {query_type}")
        
        # Fallback: extract name manually when no LLM or LLM failed
        if not specific_name:
            print("üîß Using fallback extraction")
            specific_name = self._extract_kcn_name_fallback(question)
            query_type = None  # Let query_by_specific_name handle this
            print(f"üéØ Fallback extracted: {specific_name}")
        
        if not specific_name:
            print("‚ùå Could not extract KCN name")
            return None
        
        # T√¨m th√¥ng tin KCN t·ª´ structured data
        print(f"üîç Searching for: {specific_name}")
        df_result = self.query_by_specific_name(specific_name, query_type)
        
        if df_result is None or df_result.empty:
            print(f"‚ùå No results found for: {specific_name}")
            return {
                "type": "kcn_detail_not_found",
                "message": f"Kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{specific_name}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n ho·∫∑c th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a kh√°c.",
                "query_name": specific_name
            }
        
        print(f"‚úÖ Found {len(df_result)} results")
        
        # üÜï KI·ªÇM TRA NHI·ªÄU K·∫æT QU·∫¢ TR√ôNG T√äN
        if len(df_result) > 1:
            print(f"üîÄ Multiple results found, creating choice list")
            
            # T·∫°o th√¥ng b√°o v·ªõi danh s√°ch l·ª±a ch·ªçn trong message
            choice_response = self._create_multiple_choice_response(df_result, specific_name, query_type)
            
            # Format th√†nh text message ƒë·ªÉ main.py c√≥ th·ªÉ hi·ªÉn th·ªã
            options = choice_response.get("options", [])
            message_lines = [choice_response.get("message", "")]
            message_lines.append("")  # D√≤ng tr·ªëng
            
            for i, option in enumerate(options):
                display_text = option.get("display_text", "N/A")
                message_lines.append(f"{i+1}. {display_text}")
            
            message_lines.append("")
            message_lines.append("Vui l√≤ng g·ª≠i s·ªë th·ª© t·ª± (v√≠ d·ª•: '1', '2', '3'...) ƒë·ªÉ xem th√¥ng tin chi ti·∫øt.")
            
            full_message = "\n".join(message_lines)
            
            # Tr·∫£ v·ªÅ d·∫°ng text message thay v√¨ multiple_choice ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi main.py
            return {
                "type": "kcn_detail_not_found",  # S·ª≠ d·ª•ng type n√†y ƒë·ªÉ main.py tr·∫£ v·ªÅ text
                "message": full_message,
                "query_name": specific_name,
                # L∆∞u th√¥ng tin ƒë·ªÉ x·ª≠ l√Ω sau n·∫øu c·∫ßn
                "_multiple_choice_data": choice_response
            }
        
        # Ch·ªâ c√≥ 1 k·∫øt qu·∫£ - x·ª≠ l√Ω nh∆∞ c≈©
        first_row = df_result.iloc[0]
        cols = self.columns_map
        
        kcn_info = {
            "T√™n": str(first_row.get(cols["name"], "")),
            "ƒê·ªãa ch·ªâ": str(first_row.get(cols["address"], "")),
            "T·ªânh/Th√†nh ph·ªë": str(first_row.get(cols["province"], "")),
            "Lo·∫°i": str(first_row.get(cols["type"], "")),
            "T·ªïng di·ªán t√≠ch": str(first_row.get(cols["area"], "")),
            "Gi√° thu√™ ƒë·∫•t": str(first_row.get(cols["rental_price"], "")),
            "Th·ªùi gian v·∫≠n h√†nh": str(first_row.get(cols["operation_time"], "")),
            "Ng√†nh ngh·ªÅ": str(first_row.get(cols["industry"], "")),
        }
        
        print(f"üìã KCN Info: {kcn_info['T√™n']}")
        
        # T√¨m t·ªça ƒë·ªô
        coordinates = self._match_coordinates(kcn_info["T√™n"])
        print(f"üìç Coordinates: {coordinates}")
        
        # Enhance v·ªõi RAG
        rag_analysis = self._enhance_with_rag(kcn_info, question)
        
        result = {
            "type": "kcn_detail",
            "kcn_info": kcn_info,
            "coordinates": coordinates,
            "zoom_level": 16,  # Zoom r·∫•t g·∫ßn ƒë·ªÉ th·∫•y chi ti·∫øt v·ªã tr√≠
            "matched_name": kcn_info["T√™n"],
            "query_name": specific_name,
            "message": f"Th√¥ng tin chi ti·∫øt v·ªÅ {kcn_info['T√™n']}"
        }
        
        # Th√™m RAG analysis n·∫øu c√≥
        if rag_analysis:
            result["rag_analysis"] = rag_analysis
            result["has_rag"] = True
            print("‚úÖ Added RAG analysis")
        else:
            result["has_rag"] = False
            print("‚ö†Ô∏è No RAG analysis")
        
        print("‚úÖ KCN detail query processed successfully")
        return result

    def _extract_kcn_name_fallback(self, question: str) -> Optional[str]:
        """
        Fallback method ƒë·ªÉ tr√≠ch xu·∫•t t√™n KCN/CCN khi kh√¥ng c√≥ LLM
        """
        import re
        
        question_clean = question.strip()
        
        # Pattern ƒë·∫∑c bi·ªát cho "Detail KCN/CCN [t√™n]"
        detail_match = re.search(r'detail\s+(kcn|ccn|khu c√¥ng nghi·ªáp|c·ª•m c√¥ng nghi·ªáp)\s+(.+?)(?:\s*$|\s*\?)', question_clean, re.IGNORECASE)
        if detail_match:
            kcn_type = detail_match.group(1).lower()
            kcn_name = detail_match.group(2).strip()
            if kcn_type in ['kcn', 'khu c√¥ng nghi·ªáp']:
                return f"khu c√¥ng nghi·ªáp {kcn_name}"
            else:
                return f"c·ª•m c√¥ng nghi·ªáp {kcn_name}"
        
        # Pattern 1: "v·ªÅ [t√™n KCN]"
        match = re.search(r'v·ªÅ\s+(.+?)(?:\s*$|\s*\?)', question_clean, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        
        # Pattern 2: Ch·ªâ c√≥ "KCN/CCN + t√™n" (pattern ƒë∆°n gi·∫£n)
        simple_patterns = [
            r'^(khu c√¥ng nghi·ªáp|kcn)\s+(.+?)(?:\s*$|\s*\?)',
            r'^(c·ª•m c√¥ng nghi·ªáp|ccn)\s+(.+?)(?:\s*$|\s*\?)'
        ]
        
        for pattern in simple_patterns:
            match = re.search(pattern, question_clean, re.IGNORECASE)
            if match:
                kcn_type = match.group(1).lower()
                kcn_name = match.group(2).strip()
                return f"{kcn_type} {kcn_name}"
        
        # Pattern 3: T√¨m t√™n c√≥ ch·ª©a KCN/CCN keywords trong c√¢u
        kcn_patterns = [
            r'(khu c√¥ng nghi·ªáp[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(kcn[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(c·ª•m c√¥ng nghi·ªáp[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)',
            r'(ccn[\w\s\-]+?)(?:\s*$|\s*\?|·ªü|t·∫°i)'
        ]
        
        for pattern in kcn_patterns:
            match = re.search(pattern, question_clean, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None
    # ==========================================================
    # üÜï IMPROVED KCN DETAIL QUERY WITH MULTIPLE CHOICE SUPPORT
    # ==========================================================
    
    def process_kcn_detail_query_with_multiple_choice(self, question: str) -> Optional[Dict]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi tra c·ª©u chi ti·∫øt KCN/CCN v·ªõi h·ªó tr·ª£ multiple choice
        
        Returns:
            - N·∫øu c√≥ 1 k·∫øt qu·∫£: {"type": "kcn_detail", "kcn_info": {...}, ...}
            - N·∫øu c√≥ nhi·ªÅu k·∫øt qu·∫£: {"type": "kcn_multiple_choice", "options": [...], ...}
            - N·∫øu kh√¥ng t√¨m th·∫•y: {"type": "kcn_detail_not_found", "message": "..."}
        """
        print(f"üîç Processing KCN detail query: {question}")
        
        if not self.is_kcn_detail_query(question):
            print("‚ùå Not a KCN detail query")
            return None
        
        # S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† tr√≠ch xu·∫•t t√™n KCN
        specific_name = None
        query_type = None
        
        if self.llm:
            print("ü§ñ Using LLM for analysis")
            analysis = self._analyze_query_with_llm(question)
            
            if not analysis.get("is_industrial_query", False):
                print("‚ùå LLM says not industrial query")
                return None
            
            if analysis.get("search_type") == "specific_name":
                specific_name = analysis.get("specific_name")
                query_type = analysis.get("query_type")
                print(f"üéØ LLM extracted: {specific_name}, type: {query_type}")
        
        # Fallback: extract name manually when no LLM or LLM failed
        if not specific_name:
            print("üîß Using fallback extraction")
            specific_name = self._extract_kcn_name_fallback(question)
            query_type = None  # Let query_by_specific_name handle this
            print(f"üéØ Fallback extracted: {specific_name}")
        
        if not specific_name:
            print("‚ùå Could not extract KCN name")
            return None
        
        # T√¨m th√¥ng tin KCN t·ª´ structured data
        print(f"üîç Searching for: {specific_name}")
        df_result = self.query_by_specific_name(specific_name, query_type)
        
        if df_result is None or df_result.empty:
            print(f"‚ùå No results found for: {specific_name}")
            return {
                "type": "kcn_detail_not_found",
                "message": f"Kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{specific_name}'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n ho·∫∑c th·ª≠ t√¨m ki·∫øm v·ªõi t·ª´ kh√≥a kh√°c.",
                "query_name": specific_name
            }
        
        print(f"‚úÖ Found {len(df_result)} results")
        
        # üÜï KI·ªÇM TRA NHI·ªÄU K·∫æT QU·∫¢ TR√ôNG T√äN
        if len(df_result) > 1:
            print(f"üîÄ Multiple results found, creating choice list")
            return self._create_multiple_choice_response(df_result, specific_name, query_type)
        
        # Ch·ªâ c√≥ 1 k·∫øt qu·∫£ - tr·∫£ v·ªÅ chi ti·∫øt nh∆∞ c≈©
        return self._create_single_kcn_detail_response(df_result.iloc[0], specific_name, question)

    def _create_single_kcn_detail_response(self, row, specific_name: str, question: str) -> Dict:
        """
        T·∫°o response cho 1 KCN duy nh·∫•t
        """
        cols = self.columns_map
        
        kcn_info = {
            "T√™n": str(row.get(cols["name"], "")),
            "ƒê·ªãa ch·ªâ": str(row.get(cols["address"], "")),
            "T·ªânh/Th√†nh ph·ªë": str(row.get(cols["province"], "")),
            "Lo·∫°i": str(row.get(cols["type"], "")),
            "T·ªïng di·ªán t√≠ch": str(row.get(cols["area"], "")),
            "Gi√° thu√™ ƒë·∫•t": str(row.get(cols["rental_price"], "")),
            "Th·ªùi gian v·∫≠n h√†nh": str(row.get(cols["operation_time"], "")),
            "Ng√†nh ngh·ªÅ": str(row.get(cols["industry"], "")),
        }
        
        print(f"üìã KCN Info: {kcn_info['T√™n']}")
        
        # T√¨m t·ªça ƒë·ªô
        coordinates = self._match_coordinates(kcn_info["T√™n"])
        print(f"üìç Coordinates: {coordinates}")
        
        # Enhance v·ªõi RAG
        rag_analysis = self._enhance_with_rag(kcn_info, question)
        
        result = {
            "type": "kcn_detail",
            "kcn_info": kcn_info,
            "coordinates": coordinates,
            "zoom_level": 16,  # Zoom r·∫•t g·∫ßn ƒë·ªÉ th·∫•y chi ti·∫øt v·ªã tr√≠
            "matched_name": kcn_info["T√™n"],
            "query_name": specific_name,
            "message": f"Th√¥ng tin chi ti·∫øt v·ªÅ {kcn_info['T√™n']}"
        }
        
        # Th√™m RAG analysis n·∫øu c√≥
        if rag_analysis:
            result["rag_analysis"] = rag_analysis
            result["has_rag"] = True
            print("‚úÖ Added RAG analysis")
        else:
            result["has_rag"] = False
            print("‚ö†Ô∏è No RAG analysis")
        
        print("‚úÖ KCN detail query processed successfully")
        return result